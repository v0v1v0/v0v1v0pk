<div class="container">

<table style="width: 100%;"><tr>
<td>KPCgraph</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Kernel partial correlation with geometric graphs</h2>

<h3>Description</h3>

<p>Calculate the kernel partial correlation (KPC) coefficient with directed K-nearest neighbor (K-NN) graph or minimum spanning tree (MST).
</p>


<h3>Usage</h3>

<pre><code class="language-R">KPCgraph(
  Y,
  X,
  Z,
  k = kernlab::rbfdot(1/(2 * stats::median(stats::dist(Y))^2)),
  Knn = 1,
  trans_inv = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>
<p>a matrix (n by dy)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>a matrix (n by dx) or <code>NULL</code> if <code class="reqn">X</code> is empty</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Z</code></td>
<td>
<p>a matrix (n by dz)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>a function <code class="reqn">k(y, y')</code> of class <code>kernel</code>. It can be the kernel implemented in <code>kernlab</code> e.g., Gaussian kernel: <code>rbfdot(sigma = 1)</code>, linear kernel: <code>vanilladot()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Knn</code></td>
<td>
<p>a positive integer indicating the number of nearest neighbor to use; or "MST". A small Knn (e.g., Knn=1) is recommended for an accurate estimate of the population KPC.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trans_inv</code></td>
<td>
<p>TRUE or FALSE. Is <code class="reqn">k(y, y)</code> free of <code class="reqn">y</code>?</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The kernel partial correlation squared (KPC) measures the conditional dependence
between <code class="reqn">Y</code> and <code class="reqn">Z</code> given <code class="reqn">X</code>, based on an i.i.d. sample of <code class="reqn">(Y, Z, X)</code>.
It converges to the population quantity (depending on the kernel) which is between 0 and 1.
A small value indicates low conditional dependence between <code class="reqn">Y</code> and <code class="reqn">Z</code> given <code class="reqn">X</code>, and
a large value indicates stronger conditional dependence.
If <code>X == NULL</code>, it returns the <code>KMAc(Y,Z,k,Knn)</code>, which measures the unconditional dependence between <code class="reqn">Y</code> and <code class="reqn">Z</code>.
Euclidean distance is used for computing the K-NN graph and the MST.
MST in practice often achieves similar performance as the 2-NN graph. A small K is recommended for the K-NN graph for an accurate estimate of the population KPC,
while if KPC is used as a test statistic for conditional independence, a larger K can be beneficial.
</p>


<h3>Value</h3>

<p>The algorithm returns a real number which is the estimated KPC.
</p>


<h3>See Also</h3>

<p><code>KPCRKHS</code>, <code>KMAc</code>, <code>Klin</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">library(kernlab)
n = 2000
x = rnorm(n)
z = rnorm(n)
y = x + z + rnorm(n,1,1)
KPCgraph(y,x,z,vanilladot(),Knn=1,trans_inv=FALSE)

n = 1000
x = runif(n)
z = runif(n)
y = (x + z) %% 1
KPCgraph(y,x,z,rbfdot(5),Knn="MST",trans_inv=TRUE)

discrete_ker = function(y1,y2) {
    if (y1 == y2) return(1)
    return(0)
}
class(discrete_ker) &lt;- "kernel"
set.seed(1)
n = 2000
x = rnorm(n)
z = rnorm(n)
y = rep(0,n)
for (i in 1:n) y[i] = sample(c(1,0),1,prob = c(exp(-z[i]^2/2),1-exp(-z[i]^2/2)))
KPCgraph(y,x,z,discrete_ker,1)
##0.330413
</code></pre>


</div>