<div class="container">

<table style="width: 100%;"><tr>
<td>layer_bidirectional</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Bidirectional wrapper for RNNs.</h2>

<h3>Description</h3>

<p>Bidirectional wrapper for RNNs.
</p>


<h3>Usage</h3>

<pre><code class="language-R">layer_bidirectional(
  object,
  layer,
  merge_mode = "concat",
  weights = NULL,
  backward_layer = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>Object to compose the layer with. A tensor, array, or sequential model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>layer</code></td>
<td>
<p><code>RNN</code> instance, such as
<code>layer_lstm()</code> or <code>layer_gru()</code>.
It could also be a <code>Layer()</code> instance
that meets the following criteria:
</p>

<ol>
<li>
<p> Be a sequence-processing layer (accepts 3D+ inputs).
</p>
</li>
<li>
<p> Have a <code>go_backwards</code>, <code>return_sequences</code> and <code>return_state</code>
attribute (with the same semantics as for the <code>RNN</code> class).
</p>
</li>
<li>
<p> Have an <code>input_spec</code> attribute.
</p>
</li>
<li>
<p> Implement serialization via <code>get_config()</code> and <code>from_config()</code>.
Note that the recommended way to create new RNN layers is to write a
custom RNN cell and use it with <code>layer_rnn()</code>, instead of
subclassing with <code>Layer()</code> directly.
When <code>return_sequences</code> is <code>TRUE</code>, the output of the masked
timestep will be zero regardless of the layer's original
<code>zero_output_for_mask</code> value.
</p>
</li>
</ol>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>merge_mode</code></td>
<td>
<p>Mode by which outputs of the forward and backward RNNs
will be combined. One of <code style="white-space: pre;">⁠{"sum", "mul", "concat", "ave", NULL}⁠</code>.
If <code>NULL</code>, the outputs will not be combined,
they will be returned as a list. Defaults to <code>"concat"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>see description</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>backward_layer</code></td>
<td>
<p>Optional <code>RNN</code>,
or <code>Layer()</code> instance to be used to handle
backwards input processing.
If <code>backward_layer</code> is not provided, the layer instance passed
as the <code>layer</code> argument will be used to generate the backward layer
automatically.
Note that the provided <code>backward_layer</code> layer should have properties
matching those of the <code>layer</code> argument, in particular
it should have the same values for <code>stateful</code>, <code>return_states</code>,
<code>return_sequences</code>, etc. In addition, <code>backward_layer</code>
and <code>layer</code> should have different <code>go_backwards</code> argument values.
A <code>ValueError</code> will be raised if these requirements are not met.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>For forward/backward compatability.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>The return value depends on the value provided for the first argument.
If  <code>object</code> is:
</p>

<ul>
<li>
<p> a <code>keras_model_sequential()</code>, then the layer is added to the sequential model
(which is modified in place). To enable piping, the sequential model is also
returned, invisibly.
</p>
</li>
<li>
<p> a <code>keras_input()</code>, then the output tensor from calling <code>layer(input)</code> is returned.
</p>
</li>
<li> <p><code>NULL</code> or missing, then a <code>Layer</code> instance is returned.
</p>
</li>
</ul>
<h3>Call Arguments</h3>

<p>The call arguments for this layer are the same as those of the
wrapped RNN layer. Beware that when passing the <code>initial_state</code>
argument during the call of this layer, the first half in the
list of elements in the <code>initial_state</code> list will be passed to
the forward RNN call and the last half in the list of elements
will be passed to the backward RNN call.
</p>


<h3>Note</h3>

<p>instantiating a <code>Bidirectional</code> layer from an existing RNN layer
instance will not reuse the weights state of the RNN layer instance – the
<code>Bidirectional</code> layer will have freshly initialized weights.
</p>


<h3>Examples</h3>

<div class="sourceCode r"><pre>model &lt;- keras_model_sequential(input_shape = c(5, 10)) %&gt;%
  layer_bidirectional(layer_lstm(units = 10, return_sequences = TRUE)) %&gt;%
  layer_bidirectional(layer_lstm(units = 10)) %&gt;%
  layer_dense(5, activation = "softmax")

model %&gt;% compile(loss = "categorical_crossentropy",
                  optimizer = "rmsprop")

# With custom backward layer
forward_layer &lt;- layer_lstm(units = 10, return_sequences = TRUE)
backward_layer &lt;- layer_lstm(units = 10, activation = "relu",
                             return_sequences = TRUE, go_backwards = TRUE)

model &lt;- keras_model_sequential(input_shape = c(5, 10)) %&gt;%
  bidirectional(forward_layer, backward_layer = backward_layer) %&gt;%
  layer_dense(5, activation = "softmax")

model %&gt;% compile(loss = "categorical_crossentropy",
                  optimizer = "rmsprop")
</pre></div>


<h3>States</h3>

<p>A <code>Bidirectional</code> layer instance has property <code>states</code>, which you can access
with <code>layer$states</code>. You can also reset states using <code>reset_state()</code>
</p>


<h3>See Also</h3>


<ul><li> <p><a href="https://keras.io/api/layers/recurrent_layers/bidirectional#bidirectional-class">https://keras.io/api/layers/recurrent_layers/bidirectional#bidirectional-class</a>
</p>
</li></ul>
<p>Other rnn layers: <br><code>layer_conv_lstm_1d()</code> <br><code>layer_conv_lstm_2d()</code> <br><code>layer_conv_lstm_3d()</code> <br><code>layer_gru()</code> <br><code>layer_lstm()</code> <br><code>layer_rnn()</code> <br><code>layer_simple_rnn()</code> <br><code>layer_time_distributed()</code> <br><code>rnn_cell_gru()</code> <br><code>rnn_cell_lstm()</code> <br><code>rnn_cell_simple()</code> <br><code>rnn_cells_stack()</code> <br></p>
<p>Other layers: <br><code>Layer()</code> <br><code>layer_activation()</code> <br><code>layer_activation_elu()</code> <br><code>layer_activation_leaky_relu()</code> <br><code>layer_activation_parametric_relu()</code> <br><code>layer_activation_relu()</code> <br><code>layer_activation_softmax()</code> <br><code>layer_activity_regularization()</code> <br><code>layer_add()</code> <br><code>layer_additive_attention()</code> <br><code>layer_alpha_dropout()</code> <br><code>layer_attention()</code> <br><code>layer_average()</code> <br><code>layer_average_pooling_1d()</code> <br><code>layer_average_pooling_2d()</code> <br><code>layer_average_pooling_3d()</code> <br><code>layer_batch_normalization()</code> <br><code>layer_category_encoding()</code> <br><code>layer_center_crop()</code> <br><code>layer_concatenate()</code> <br><code>layer_conv_1d()</code> <br><code>layer_conv_1d_transpose()</code> <br><code>layer_conv_2d()</code> <br><code>layer_conv_2d_transpose()</code> <br><code>layer_conv_3d()</code> <br><code>layer_conv_3d_transpose()</code> <br><code>layer_conv_lstm_1d()</code> <br><code>layer_conv_lstm_2d()</code> <br><code>layer_conv_lstm_3d()</code> <br><code>layer_cropping_1d()</code> <br><code>layer_cropping_2d()</code> <br><code>layer_cropping_3d()</code> <br><code>layer_dense()</code> <br><code>layer_depthwise_conv_1d()</code> <br><code>layer_depthwise_conv_2d()</code> <br><code>layer_discretization()</code> <br><code>layer_dot()</code> <br><code>layer_dropout()</code> <br><code>layer_einsum_dense()</code> <br><code>layer_embedding()</code> <br><code>layer_feature_space()</code> <br><code>layer_flatten()</code> <br><code>layer_flax_module_wrapper()</code> <br><code>layer_gaussian_dropout()</code> <br><code>layer_gaussian_noise()</code> <br><code>layer_global_average_pooling_1d()</code> <br><code>layer_global_average_pooling_2d()</code> <br><code>layer_global_average_pooling_3d()</code> <br><code>layer_global_max_pooling_1d()</code> <br><code>layer_global_max_pooling_2d()</code> <br><code>layer_global_max_pooling_3d()</code> <br><code>layer_group_normalization()</code> <br><code>layer_group_query_attention()</code> <br><code>layer_gru()</code> <br><code>layer_hashed_crossing()</code> <br><code>layer_hashing()</code> <br><code>layer_identity()</code> <br><code>layer_integer_lookup()</code> <br><code>layer_jax_model_wrapper()</code> <br><code>layer_lambda()</code> <br><code>layer_layer_normalization()</code> <br><code>layer_lstm()</code> <br><code>layer_masking()</code> <br><code>layer_max_pooling_1d()</code> <br><code>layer_max_pooling_2d()</code> <br><code>layer_max_pooling_3d()</code> <br><code>layer_maximum()</code> <br><code>layer_mel_spectrogram()</code> <br><code>layer_minimum()</code> <br><code>layer_multi_head_attention()</code> <br><code>layer_multiply()</code> <br><code>layer_normalization()</code> <br><code>layer_permute()</code> <br><code>layer_random_brightness()</code> <br><code>layer_random_contrast()</code> <br><code>layer_random_crop()</code> <br><code>layer_random_flip()</code> <br><code>layer_random_rotation()</code> <br><code>layer_random_translation()</code> <br><code>layer_random_zoom()</code> <br><code>layer_repeat_vector()</code> <br><code>layer_rescaling()</code> <br><code>layer_reshape()</code> <br><code>layer_resizing()</code> <br><code>layer_rnn()</code> <br><code>layer_separable_conv_1d()</code> <br><code>layer_separable_conv_2d()</code> <br><code>layer_simple_rnn()</code> <br><code>layer_spatial_dropout_1d()</code> <br><code>layer_spatial_dropout_2d()</code> <br><code>layer_spatial_dropout_3d()</code> <br><code>layer_spectral_normalization()</code> <br><code>layer_string_lookup()</code> <br><code>layer_subtract()</code> <br><code>layer_text_vectorization()</code> <br><code>layer_tfsm()</code> <br><code>layer_time_distributed()</code> <br><code>layer_torch_module_wrapper()</code> <br><code>layer_unit_normalization()</code> <br><code>layer_upsampling_1d()</code> <br><code>layer_upsampling_2d()</code> <br><code>layer_upsampling_3d()</code> <br><code>layer_zero_padding_1d()</code> <br><code>layer_zero_padding_2d()</code> <br><code>layer_zero_padding_3d()</code> <br><code>rnn_cell_gru()</code> <br><code>rnn_cell_lstm()</code> <br><code>rnn_cell_simple()</code> <br><code>rnn_cells_stack()</code> <br></p>


</div>