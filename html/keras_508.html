<div class="container">

<table style="width: 100%;"><tr>
<td>multi_gpu_model</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>(Deprecated) Replicates a model on different GPUs.</h2>

<h3>Description</h3>

<p>(Deprecated) Replicates a model on different GPUs.
</p>


<h3>Usage</h3>

<pre><code class="language-R">multi_gpu_model(model, gpus = NULL, cpu_merge = TRUE, cpu_relocation = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>A Keras model instance. To avoid OOM errors,
this model could have been built on CPU, for instance
(see usage example below).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gpus</code></td>
<td>
<p><code>NULL</code> to use all available GPUs (default). Integer &gt;= 2 or
list of integers, number of GPUs or list of GPU IDs on which to create
model replicas.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cpu_merge</code></td>
<td>
<p>A boolean value to identify whether to force
merging model weights under the scope of the CPU or not.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cpu_relocation</code></td>
<td>
<p>A boolean value to identify whether to
create the model's weights under the scope of the CPU.
If the model is not defined under any preceding device
scope, you can still rescue it by activating this option.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Specifically, this function implements single-machine
multi-GPU data parallelism. It works in the following way:
</p>

<ul>
<li>
<p> Divide the model's input(s) into multiple sub-batches.
</p>
</li>
<li>
<p> Apply a model copy on each sub-batch. Every model copy
is executed on a dedicated GPU.
</p>
</li>
<li>
<p> Concatenate the results (on CPU) into one big batch.
</p>
</li>
</ul>
<p>E.g. if your <code>batch_size</code> is 64 and you use <code>gpus=2</code>,
then we will divide the input into 2 sub-batches of 32 samples,
process each sub-batch on one GPU, then return the full
batch of 64 processed samples.
</p>
<p>This induces quasi-linear speedup on up to 8 GPUs.
</p>
<p>This function is only available with the TensorFlow backend
for the time being.
</p>


<h3>Value</h3>

<p>A Keras model object which can be used just like the initial
<code>model</code> argument, but which distributes its workload on multiple GPUs.
</p>


<h3>Model Saving</h3>

<p>To save the multi-gpu model, use <code>save_model_hdf5()</code> or
<code>save_model_weights_hdf5()</code> with the template model (the argument you
passed to <code>multi_gpu_model</code>), rather than the model returned
by <code>multi_gpu_model</code>.
</p>


<h3>Note</h3>

<p>This function is deprecated and has been removed from tensorflow on
2020-04-01. To distribute your training across all available GPUS,
you can use <code>tensorflow::tf$distribute$MirroredStrategy()</code>
by creating your model like this:
</p>
<div class="sourceCode r"><pre>strategy &lt;- tensorflow::tf$distribute$MirroredStrategy()
with(strategy$scope(), {
  model &lt;- application_xception(
    weights = NULL,
    input_shape = c(height, width, 3),
    classes = num_classes
})
</pre></div>


<h3>See Also</h3>

<p>Other model functions: 
<code>compile.keras.engine.training.Model()</code>,
<code>evaluate.keras.engine.training.Model()</code>,
<code>evaluate_generator()</code>,
<code>fit.keras.engine.training.Model()</code>,
<code>fit_generator()</code>,
<code>get_config()</code>,
<code>get_layer()</code>,
<code>keras_model()</code>,
<code>keras_model_sequential()</code>,
<code>pop_layer()</code>,
<code>predict.keras.engine.training.Model()</code>,
<code>predict_generator()</code>,
<code>predict_on_batch()</code>,
<code>predict_proba()</code>,
<code>summary.keras.engine.training.Model()</code>,
<code>train_on_batch()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 

library(keras)
library(tensorflow)

num_samples &lt;- 1000
height &lt;- 224
width &lt;- 224
num_classes &lt;- 1000

# Instantiate the base model (or "template" model).
# We recommend doing this with under a CPU device scope,
# so that the model's weights are hosted on CPU memory.
# Otherwise they may end up hosted on a GPU, which would
# complicate weight sharing.
with(tf$device("/cpu:0"), {
  model &lt;- application_xception(
    weights = NULL,
    input_shape = c(height, width, 3),
    classes = num_classes
  )
})

# Replicates the model on 8 GPUs.
# This assumes that your machine has 8 available GPUs.
parallel_model &lt;- multi_gpu_model(model, gpus = 8)
parallel_model %&gt;% compile(
  loss = "categorical_crossentropy",
  optimizer = "rmsprop"
)

# Generate dummy data.
x &lt;- array(runif(num_samples * height * width*3),
           dim = c(num_samples, height, width, 3))
y &lt;- array(runif(num_samples * num_classes),
           dim = c(num_samples, num_classes))

# This `fit` call will be distributed on 8 GPUs.
# Since the batch size is 256, each GPU will process 32 samples.
parallel_model %&gt;% fit(x, y, epochs = 20, batch_size = 256)

# Save model via the template model (which shares the same weights):
model %&gt;% save_model_hdf5("my_model.h5")

## End(Not run)

</code></pre>


</div>