<div class="container">

<table style="width: 100%;"><tr>
<td>gp</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Gaussian Process Model
</h2>

<h3>Description</h3>

<p>Gaussian Process model.
</p>


<h3>Usage</h3>

<pre><code class="language-R">
gp(formula, data, inputs = inputNames(cov), cov, estim = TRUE, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>

<p>A formula with a left-hand side specifying the response name, and the
right-hand side the trend covariates (see examples below). Factors
are not allowed neither as response nor as covariates.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>

<p>A data frame containing the response, the inputs specified in
<code>inputs</code>, and all the trend variables required in
<code>formula</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>inputs</code></td>
<td>

<p>A character vector giving the names of the inputs.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cov</code></td>
<td>

<p>A covariance kernel object or call.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>estim</code></td>
<td>

<p>Logical. If <code>TRUE</code>, the model parameters are estimated by
Maximum Likelihood. The initial values can then be specified using
the <code>parCovIni</code> and <code>varNoiseIni</code> arguments of
<code>mle,covAll-method</code> passed though <code>dots</code>. If
<code>FALSE</code>, a simple Generalized Least Squares estimation will be
used, see <code>gls,covAll-method</code>. Then the value of
<code>varNoise</code> must be given and passed through <code>dots</code> in case
<code>noise</code> is <code>TRUE</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>

<p>Other arguments passed to the estimation method.  This will be the
<code>mle,covAll-method</code> if <code>estim</code> is <code>TRUE</code> or
<code>gls,covAll-method</code> if <code>estim</code> is <code>FALSE</code>. In
the first case, the arguments will typically include
<code>varNoiseIni</code>. In the second case, they will typically include
<code>varNoise</code>. Note that a logical <code>noise</code> can be used in the
<code>"mle"</code> case. In both cases, the arguments <code>y</code>, <code>X</code>,
<code>F</code> can not be used since they are automatically passed.
</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A list object which is given the S3 class <code>"gp"</code>. The list content
is very likely to change, and should be used through methods.
</p>


<h3>Note</h3>

<p>When <code>estim</code> is <code>TRUE</code>, the covariance object in <code>cov</code>
is expected to provide a gradient when used to compute a covariance
matrix, since the default value of <code>compGrad</code> it <code>TRUE</code>, 
see <code>mle,covAll-method</code>.  
</p>


<h3>Author(s)</h3>

<p>Y. Deville, D. Ginsbourger, O. Roustant
</p>


<h3>See Also</h3>

<p><code>mle,covAll-method</code> for a detailed example of
maximum-likelihood estimation.
</p>


<h3>Examples</h3>

<pre><code class="language-R">## ==================================================================
## Example 1. Data sampled from a GP model with a known covTS object
## ==================================================================
set.seed(1234)
myCov &lt;- covTS(inputs = c("Temp", "Humid"),
               kernel = "k1Matern5_2",
               dep = c(range = "input"),
               value = c(range = 0.4))
## change coefficients (variances)
coef(myCov) &lt;- c(0.5, 0.8, 2, 16)
d &lt;- myCov@d; n &lt;- 20
## design matrix
X &lt;- matrix(runif(n*d), nrow = n, ncol = d)
colnames(X) &lt;- inputNames(myCov)
## generate the GP realization
myGp &lt;- gp(formula = y ~ 1, data = data.frame(y = rep(0, n), X), 
            cov = myCov, estim = FALSE,
            beta = 10, varNoise = 0.05)
y &lt;- simulate(myGp, cond = FALSE)$sim

## parIni: add noise to true parameters
parCovIni &lt;- coef(myCov)
parCovIni[] &lt;- 0.9 * parCovIni[] +  0.1 * runif(length(parCovIni))
coefLower(myCov) &lt;- rep(1e-2, 4)
coefUpper(myCov) &lt;- c(5, 5, 20, 20)
est &lt;- gp(y ~ 1, data = data.frame(y = y, X),
          cov = myCov, 
          noise = TRUE,
          varNoiseLower = 1e-2,
          varNoiseIni = 1.0,
          parCovIni = parCovIni) 
summary(est)
coef(est)

## =======================================================================
## Example 2. Predicting an additive function with an additive GP model
## =======================================================================

## Not run: 
    
    addfun6d &lt;- function(x){
       res &lt;- x[1]^3 + cos(pi * x[2]) + abs(x[3]) * sin(x[3]^2) +
           3 * x[4]^3 + 3 * cos(pi * x[5]) + 3 * abs(x[6]) * sin(x[6]^2)
    }

    ## 'Fit' is for the learning set, 'Val' for the validation set
    set.seed(123)
    nFit &lt;- 50   
    nVal &lt;- 200
    d &lt;- 6 
    inputs &lt;- paste("x", 1L:d, sep = "")

    ## create design matrices with DiceDesign package 
    require(DiceDesign)
    require(DiceKriging)
    set.seed(0)
    dataFitIni &lt;- DiceDesign::lhsDesign(nFit, d)$design 
    dataValIni &lt;- DiceDesign::lhsDesign(nVal, d)$design 
    dataFit &lt;- DiceDesign::maximinSA_LHS(dataFitIni)$design
    dataVal &lt;- DiceDesign::maximinSA_LHS(dataValIni)$design

    colnames(dataFit) &lt;- colnames(dataVal) &lt;- inputs
    testfun &lt;- addfun6d
    dataFit &lt;- data.frame(dataFit, y = apply(dataFit, 1, testfun))
    dataVal &lt;- data.frame(dataVal, y = apply(dataVal, 1, testfun))

    ## Creation of "CovTS" object with one range by input
    myCov &lt;- covTS(inputs = inputs, d = d, kernel = "k1Matern3_2", 
                   dep = c(range = "input"))

    ## Creation of a gp object
    fitgp &lt;- gp(formula = y ~ 1, data = dataFit, 
                cov = myCov, noise = TRUE, 
                parCovIni = rep(1, 2*d),
                parCovLower = c(rep(1e-4, 2*d)),
                parCovUpper = c(rep(5, d), rep(10,d)))
 
    predTS &lt;- predict(fitgp, newdata = as.matrix(dataVal[ , inputs]), type = "UK")$mean

    ## Classical tensor product kernel as a reference for comparison
    fitRef &lt;- DiceKriging::km(formula = ~1,
                              design = dataFit[ , inputs],
                              response = dataFit$y,  covtype="matern3_2")
    predRef &lt;- predict(fitRef,
                       newdata = as.matrix(dataVal[ , inputs]),
                       type = "UK")$mean
    ## Compare TS and Ref
    RMSE &lt;- data.frame(TS = sqrt(mean((dataVal$y - predTS)^2)),
                       Ref = sqrt(mean((dataVal$y - predRef)^2)),
                       row.names = "RMSE")
    print(RMSE)

    Comp &lt;- data.frame(y = dataVal$y, predTS, predRef)
    plot(predRef ~ y, data = Comp, col = "black", pch = 4,
         xlab = "True", ylab = "Predicted",
         main = paste("Prediction on a validation set (nFit = ",
                      nFit, ", nVal = ", nVal, ").", sep = ""))
    points(predTS ~ y, data = Comp, col = "red", pch = 20)
    abline(a = 0, b = 1, col = "blue", lty = "dotted")
    legend("bottomright", pch = c(4, 20), col = c("black", "red"),
           legend = c("Ref", "Tensor Sum"))

## End(Not run)

##=======================================================================
## Example 3: a 'covMan' kernel with 3 implementations
##=======================================================================

d &lt;- 4

## -- Define a 4-dimensional covariance structure with a kernel in R

myGaussFunR &lt;- function(x1, x2, par) { 
    h &lt;- (x1 - x2) / par[1]
    SS2 &lt;- sum(h^2)
    d2 &lt;- exp(-SS2)
    kern &lt;- par[2] * d2
    d1 &lt;- 2 * kern * SS2 / par[1]            
    attr(kern, "gradient") &lt;- c(theta = d1,  sigma2 = d2)
    return(kern)
}

myGaussR &lt;- covMan(kernel = myGaussFunR,
                   hasGrad = TRUE,
                   d = d,
                   parLower = c(theta = 0.0, sigma2 = 0.0),
                   parUpper = c(theta = Inf, sigma2 = Inf),
                   parNames = c("theta", "sigma2"),
                   label = "Gaussian kernel: R implementation")

## -- The same, still in R, but with a kernel admitting matrices as arguments

myGaussFunRVec &lt;- function(x1, x2, par) { 
    # x1, x2 : matrices with same number of columns 'd' (dimension)
    n &lt;- nrow(x1)
    d &lt;- ncol(x1)     
    SS2 &lt;- 0  
    for (j in 1:d){
        Aj &lt;- outer(x1[ , j], x2[ , j], "-")
        Hj2 &lt;- (Aj / par[1])^2
        SS2 &lt;- SS2 + Hj2
    }
    D2 &lt;- exp(-SS2)
    kern &lt;- par[2] * D2
    D1 &lt;- 2 * kern * SS2 / par[1] 
    attr(kern, "gradient") &lt;- list(theta = D1,  sigma2 = D2)
  
    return(kern)
}

myGaussRVec &lt;- covMan(
    kernel = myGaussFunRVec,
    hasGrad = TRUE,
    acceptMatrix = TRUE,
    d = d,
    parLower = c(theta = 0.0, sigma2 = 0.0),
    parUpper = c(theta = Inf, sigma2 = Inf),
    parNames = c("theta", "sigma2"),
    label = "Gaussian kernel: vectorised R implementation"
)

## -- The same, with inlined C code
## (see also another example with Rcpp by typing: ?kergp).

if (require(inline)) {

    kernCode &lt;- "
       SEXP kern, dkern;
       int nprotect = 0, d;
       double SS2 = 0.0, d2, z, *rkern, *rdkern;

       d = LENGTH(x1);
       PROTECT(kern = allocVector(REALSXP, 1)); nprotect++;
       PROTECT(dkern = allocVector(REALSXP, 2)); nprotect++;
       rkern = REAL(kern);
       rdkern = REAL(dkern);

       for (int i = 0; i &lt; d; i++) {
          z = ( REAL(x1)[i] - REAL(x2)[i] ) / REAL(par)[0];
          SS2 += z * z; 
       }

       d2 = exp(-SS2);
       rkern[0] = REAL(par)[1] * d2;
       rdkern[1] =  d2; 
       rdkern[0] =  2 * rkern[0] * SS2 / REAL(par)[0];

       SET_ATTR(kern, install(\"gradient\"), dkern);
       UNPROTECT(nprotect);
       return kern;
   "
    myGaussFunC &lt;- cfunction(sig = signature(x1 = "numeric", x2 = "numeric",
                                          par = "numeric"),
                             body = kernCode)

    myGaussC &lt;- covMan(kernel = myGaussFunC,
                       hasGrad = TRUE,
                       d = d,
                       parLower = c(theta = 0.0, sigma2 = 0.0),
                       parUpper = c(theta = Inf, sigma2 = Inf),
                       parNames = c("theta", "sigma2"),
                       label = "Gaussian kernel: C/inline implementation")

}

## == Simulate data for covMan and trend ==

n &lt;- 100; p &lt;- d + 1
X &lt;- matrix(runif(n * d), nrow = n)
colnames(X) &lt;- inputNames(myGaussRVec)
design &lt;- data.frame(X)
coef(myGaussRVec) &lt;- myPar &lt;- c(theta = 0.5, sigma2 = 2)
myGp &lt;- gp(formula = y ~ 1, data = data.frame(y = rep(0, n), design), 
            cov = myGaussRVec, estim = FALSE,
            beta = 0, varNoise = 1e-8)
y &lt;- simulate(myGp, cond = FALSE)$sim
F &lt;- matrix(runif(n * p), nrow = n, ncol = p)
beta &lt;- (1:p) / p
y &lt;- tcrossprod(F, t(beta)) + y

## == ML estimation. ==
tRVec &lt;- system.time(
    resRVec &lt;- gp(formula = y ~ ., data = data.frame(y = y, design),
                  cov = myGaussRVec,
                  compGrad = TRUE, 
                  parCovIni = c(0.5, 0.5), varNoiseLower = 1e-4,
                  parCovLower = c(1e-5, 1e-5), parCovUpper = c(Inf, Inf))
)

summary(resRVec)
coef(resRVec)
pRVec &lt;- predict(resRVec, newdata = design, type = "UK")    
tAll &lt;- tRVec
coefAll &lt;- coef(resRVec)
## compare time required by the 3 implementations
## Not run: 
    tR &lt;- system.time(
        resR &lt;- gp(formula = y ~ ., data = data.frame(y = y, design),
                   cov = myGaussR,
                   compGrad = TRUE, 
                   parCovIni = c(0.5, 0.5), varNoiseLower = 1e-4,
                   parCovLower = c(1e-5, 1e-5), parCovUpper = c(Inf, Inf))
    )
    tAll &lt;- rbind(tRVec = tAll, tR)
    coefAll &lt;- rbind(coefAll, coef(resR))
    if (require(inline)) {
        tC &lt;- system.time(
            resC &lt;- gp(formula = y ~ ., data = data.frame(y = y, design),
                       cov = myGaussC,
                       compGrad = TRUE, 
                       parCovIni = c(0.5, 0.5), varNoiseLower = 1e-4,
                       parCovLower = c(1e-5, 1e-5), parCovUpper = c(Inf, Inf))
        )
        tAll &lt;- rbind(tAll, tC)
        coefAll &lt;- rbind(coefAll, coef(resC))
    }

## End(Not run)
tAll

## rows must be identical 
coefAll

</code></pre>


</div>