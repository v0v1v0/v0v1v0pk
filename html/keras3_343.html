<div class="container">

<table style="width: 100%;"><tr>
<td>metric_categorical_focal_crossentropy</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Computes the categorical focal crossentropy loss.</h2>

<h3>Description</h3>

<p>Computes the categorical focal crossentropy loss.
</p>


<h3>Usage</h3>

<pre><code class="language-R">metric_categorical_focal_crossentropy(
  y_true,
  y_pred,
  alpha = 0.25,
  gamma = 2,
  from_logits = FALSE,
  label_smoothing = 0,
  axis = -1L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y_true</code></td>
<td>
<p>Tensor of one-hot true targets.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y_pred</code></td>
<td>
<p>Tensor of predicted targets.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>A weight balancing factor for all classes, default is <code>0.25</code> as
mentioned in the reference. It can be a list of floats or a scalar.
In the multi-class case, alpha may be set by inverse class
frequency by using <code>compute_class_weight</code> from <code>sklearn.utils</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma</code></td>
<td>
<p>A focusing parameter, default is <code>2.0</code> as mentioned in the
reference. It helps to gradually reduce the importance given to
simple examples in a smooth manner. When <code>gamma</code> = 0, there is
no focal effect on the categorical crossentropy.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>from_logits</code></td>
<td>
<p>Whether <code>y_pred</code> is expected to be a logits tensor. By
default, we assume that <code>y_pred</code> encodes a probability
distribution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>label_smoothing</code></td>
<td>
<p>Float in <code style="white-space: pre;">⁠[0, 1].⁠</code> If &gt; <code>0</code> then smooth the labels. For
example, if <code>0.1</code>, use <code>0.1 / num_classes</code> for non-target labels
and <code>0.9 + 0.1 / num_classes</code> for target labels.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>axis</code></td>
<td>
<p>Defaults to <code>-1</code>. The dimension along which the entropy is
computed.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>Categorical focal crossentropy loss value.
</p>


<h3>Examples</h3>

<div class="sourceCode r"><pre>y_true &lt;- rbind(c(0, 1, 0), c(0, 0, 1))
y_pred &lt;- rbind(c(0.05, 0.9, 0.05), c(0.1, 0.85, 0.05))
loss &lt;- loss_categorical_focal_crossentropy(y_true, y_pred)
loss
</pre></div>
<div class="sourceCode"><pre>## tf.Tensor([2.63401289e-04 6.75912094e-01], shape=(2), dtype=float64)

</pre></div>


<h3>See Also</h3>

<p>Other losses: <br><code>Loss()</code> <br><code>loss_binary_crossentropy()</code> <br><code>loss_binary_focal_crossentropy()</code> <br><code>loss_categorical_crossentropy()</code> <br><code>loss_categorical_focal_crossentropy()</code> <br><code>loss_categorical_hinge()</code> <br><code>loss_cosine_similarity()</code> <br><code>loss_ctc()</code> <br><code>loss_dice()</code> <br><code>loss_hinge()</code> <br><code>loss_huber()</code> <br><code>loss_kl_divergence()</code> <br><code>loss_log_cosh()</code> <br><code>loss_mean_absolute_error()</code> <br><code>loss_mean_absolute_percentage_error()</code> <br><code>loss_mean_squared_error()</code> <br><code>loss_mean_squared_logarithmic_error()</code> <br><code>loss_poisson()</code> <br><code>loss_sparse_categorical_crossentropy()</code> <br><code>loss_squared_hinge()</code> <br><code>loss_tversky()</code> <br><code>metric_binary_crossentropy()</code> <br><code>metric_binary_focal_crossentropy()</code> <br><code>metric_categorical_crossentropy()</code> <br><code>metric_categorical_hinge()</code> <br><code>metric_hinge()</code> <br><code>metric_huber()</code> <br><code>metric_kl_divergence()</code> <br><code>metric_log_cosh()</code> <br><code>metric_mean_absolute_error()</code> <br><code>metric_mean_absolute_percentage_error()</code> <br><code>metric_mean_squared_error()</code> <br><code>metric_mean_squared_logarithmic_error()</code> <br><code>metric_poisson()</code> <br><code>metric_sparse_categorical_crossentropy()</code> <br><code>metric_squared_hinge()</code> <br></p>
<p>Other metrics: <br><code>Metric()</code> <br><code>custom_metric()</code> <br><code>metric_auc()</code> <br><code>metric_binary_accuracy()</code> <br><code>metric_binary_crossentropy()</code> <br><code>metric_binary_focal_crossentropy()</code> <br><code>metric_binary_iou()</code> <br><code>metric_categorical_accuracy()</code> <br><code>metric_categorical_crossentropy()</code> <br><code>metric_categorical_hinge()</code> <br><code>metric_cosine_similarity()</code> <br><code>metric_f1_score()</code> <br><code>metric_false_negatives()</code> <br><code>metric_false_positives()</code> <br><code>metric_fbeta_score()</code> <br><code>metric_hinge()</code> <br><code>metric_huber()</code> <br><code>metric_iou()</code> <br><code>metric_kl_divergence()</code> <br><code>metric_log_cosh()</code> <br><code>metric_log_cosh_error()</code> <br><code>metric_mean()</code> <br><code>metric_mean_absolute_error()</code> <br><code>metric_mean_absolute_percentage_error()</code> <br><code>metric_mean_iou()</code> <br><code>metric_mean_squared_error()</code> <br><code>metric_mean_squared_logarithmic_error()</code> <br><code>metric_mean_wrapper()</code> <br><code>metric_one_hot_iou()</code> <br><code>metric_one_hot_mean_iou()</code> <br><code>metric_poisson()</code> <br><code>metric_precision()</code> <br><code>metric_precision_at_recall()</code> <br><code>metric_r2_score()</code> <br><code>metric_recall()</code> <br><code>metric_recall_at_precision()</code> <br><code>metric_root_mean_squared_error()</code> <br><code>metric_sensitivity_at_specificity()</code> <br><code>metric_sparse_categorical_accuracy()</code> <br><code>metric_sparse_categorical_crossentropy()</code> <br><code>metric_sparse_top_k_categorical_accuracy()</code> <br><code>metric_specificity_at_sensitivity()</code> <br><code>metric_squared_hinge()</code> <br><code>metric_sum()</code> <br><code>metric_top_k_categorical_accuracy()</code> <br><code>metric_true_negatives()</code> <br><code>metric_true_positives()</code> <br></p>


</div>