<div class="container">

<table style="width: 100%;"><tr>
<td>KODAMA.matrix</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Knowledge Discovery by Accuracy Maximization</h2>

<h3>Description</h3>

<p>KODAMA (KnOwledge Discovery by Accuracy MAximization) is an unsupervised and semi-supervised learning algorithm that performs feature extraction from noisy and high-dimensional data. 
</p>


<h3>Usage</h3>

<pre><code class="language-R">KODAMA.matrix (data, 
               M = 100, 
               Tcycle = 20, 
               FUN_VAR = function(x) {  ceiling(ncol(x)) }, 
               FUN_SAM = function(x) {  ceiling(nrow(x) * 0.75)},
               bagging = FALSE, 
               FUN = c("PLS-DA","KNN"), 
               f.par = 5,
               W = NULL, 
               constrain = NULL, 
               fix=NULL, 
               epsilon = 0.05,
               dims=2,
               landmarks=1000,
               neighbors=min(c(landmarks,nrow(data)))-1) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>a matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>M</code></td>
<td>

<p>number of iterative processes (step I-III).
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Tcycle</code></td>
<td>

<p>number of iterative cycles that leads to the maximization of cross-validated accuracy.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>FUN_VAR</code></td>
<td>

<p>function to select the number of variables to select randomly. By default all variable are taken.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>FUN_SAM</code></td>
<td>

<p>function to select the number of samples to select randomly. By default the 75 per cent of all samples are taken.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bagging</code></td>
<td>

<p>Should sampling be with replacement, <code>bagging = TRUE</code>. By default <code>bagging = FALSE</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>FUN</code></td>
<td>
<p>classifier to be considered. Choices are "<code>PLS-DA</code>" and "<code>KNN</code>".
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>f.par</code></td>
<td>

<p>parameters of the classifier.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>W</code></td>
<td>

<p>a vector of <code>nrow(data)</code> elements. The KODAMA procedure can be started by different initializations of the vector <code>W</code>. Without any <em>a priori</em> information the vector <code>W</code> can be initialized with each element being different from the others (<em>i.e.</em>, each sample categorized in a one-element class). Alternatively, the vector <code>W</code> can be initialized by a clustering procedure, such as <code>kmeans</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>constrain</code></td>
<td>

<p>a vector of <code>nrow(data)</code> elements. Supervised constraints can be imposed by linking some samples in such a way that if one of them is changed the remaining linked samples must change in the same way (<em>i.e.</em>, they are forced to belong to the same class) during the maximization of the cross-validation accuracy procedure. Samples with the same identifying constrain will be forced to stay together.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fix</code></td>
<td>

<p>a vector of <code>nrow(data)</code> elements. The values of this vector must to be <code>TRUE</code> or <code>FALSE</code>. By default all elements are <code>FALSE</code>. Samples with the <code>TRUE</code> fix value will not change the class label defined in <code>W</code> during the maximization of the cross-validation accuracy procedure.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>

<p>cut-off value for low proximity. High proximity are typical of intracluster relationships, whereas low proximities are expected for intercluster relationships. Very low proximities between samples are ignored by (default) setting <code>epsilon = 0.05</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dims</code></td>
<td>

<p>dimensions of the configurations of t-SNE based on the KODAMA dissimilarity matrix.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>landmarks</code></td>
<td>

<p>number of landmarks to use.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>neighbors</code></td>
<td>

<p>number of neighbors to include in the dissimilarity matrix yo pass to the <code>KODAMA.visualization</code> function.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>KODAMA consists of five steps. These can be in turn divided into two parts: (i) the maximization of cross-validated accuracy by an iterative process (step I and II), resulting in the construction of a proximity matrix (step III), and (ii) the definition of a dissimilarity matrix (step IV and V). The first part entails the core idea of KODAMA, that is, the partitioning of data guided by the maximization of the cross-validated accuracy. At the beginning of this part, a fraction of the total samples (defined by <code>FUN_SAM</code>) are randomly selected from the original data. The whole iterative process (step I-III) is repeated <code>M</code> times to average the effects owing to the randomness of the iterative procedure. Each time that this part is repeated, a different fraction of samples is selected. The second part aims at collecting and processing these results by constructing a dissimilarity matrix to provide a holistic view of the data while maintaining their intrinsic structure (steps IV and V). Then, <code>KODAMA.visualization</code> function is used to visualise the results of KODAMA dissimilarity matrix. 
</p>


<h3>Value</h3>

<p>The function returns a list with 4 items:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>dissimilarity</code></td>
<td>
<p>a dissimilarity matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>acc</code></td>
<td>
<p>a vector with the <code>M</code> cross-validated accuracies.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>proximity</code></td>
<td>
<p>a proximity matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>v</code></td>
<td>
<p>a matrix containing the all classification obtained maximizing the cross-validation accuracy.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>res</code></td>
<td>
<p>a matrix containing all classification vectors obtained through maximizing the cross-validation accuracy.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>f.par</code></td>
<td>
<p>parameters of the classifier..</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>entropy</code></td>
<td>
<p>Shannon's entropy of the KODAMA proximity matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>landpoints</code></td>
<td>
<p>indexes of the landmarks used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>original data.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>knn_Armadillo</code></td>
<td>
<p>dissimilarity matrix used as input for the <code>KODAMA.visualization</code> function.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Stefano Cacciatore and Leonardo Tenori</p>


<h3>References</h3>

<p>Cacciatore S, Luchinat C, Tenori L	<br>
Knowledge discovery by accuracy maximization.<br><em>Proc Natl Acad Sci U S A</em> 2014;111(14):5117-22. doi: 10.1073/pnas.1220873111. <a href="https://www.pnas.org/doi/10.1073/pnas.1220873111">Link</a>
<br><br>
Cacciatore S, Tenori L, Luchinat C, Bennett PR, MacIntyre DA	<br>
KODAMA: an updated R package for knowledge discovery and data mining.	<br><em>Bioinformatics</em> 2017;33(4):621-623. doi: 10.1093/bioinformatics/btw705. <a href="https://academic.oup.com/bioinformatics/article/33/4/621/2667156">Link</a>
<br><br>
L.J.P. van der Maaten and G.E. Hinton.<br>
Visualizing High-Dimensional Data Using t-SNE. <br><em>Journal of Machine Learning Research</em> 9 (Nov) : 2579-2605, 2008.
<br><br>
L.J.P. van der Maaten. <br>
Learning a Parametric Embedding by Preserving Local Structure. <br><em>In Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics (AISTATS), JMLR W&amp;CP</em> 5:384-391, 2009.
<br><br>
McInnes L, Healy J, Melville J. <br>
Umap: Uniform manifold approximation and projection for dimension reduction. <br><em>arXiv preprint</em>:1802.03426. 2018 Feb 9.
</p>


<h3>See Also</h3>

<p><code>KODAMA.visualization</code></p>


<h3>Examples</h3>

<pre><code class="language-R">

 data(iris)
 data=iris[,-5]
 labels=iris[,5]
 kk=KODAMA.matrix(data,FUN="KNN",f.par=2)
 cc=KODAMA.visualization(kk,"t-SNE")
 plot(cc,col=as.numeric(labels),cex=2)


</code></pre>


</div>