<div class="container">

<table style="width: 100%;"><tr>
<td>layer_lstm_cell</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Cell class for the LSTM layer</h2>

<h3>Description</h3>

<p>Cell class for the LSTM layer
</p>


<h3>Usage</h3>

<pre><code class="language-R">layer_lstm_cell(
  units,
  activation = "tanh",
  recurrent_activation = "sigmoid",
  use_bias = TRUE,
  kernel_initializer = "glorot_uniform",
  recurrent_initializer = "orthogonal",
  bias_initializer = "zeros",
  unit_forget_bias = TRUE,
  kernel_regularizer = NULL,
  recurrent_regularizer = NULL,
  bias_regularizer = NULL,
  kernel_constraint = NULL,
  recurrent_constraint = NULL,
  bias_constraint = NULL,
  dropout = 0,
  recurrent_dropout = 0,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>units</code></td>
<td>
<p>Positive integer, dimensionality of the output space.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>activation</code></td>
<td>
<p>Activation function to use. Default: hyperbolic tangent
(<code>tanh</code>). If you pass <code>NULL</code>, no activation is applied (ie. "linear"
activation: <code>a(x) = x</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>recurrent_activation</code></td>
<td>
<p>Activation function to use for the recurrent step.
Default: sigmoid (<code>sigmoid</code>). If you pass <code>NULL</code>, no activation is applied
(ie. "linear" activation: <code>a(x) = x</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>use_bias</code></td>
<td>
<p>Boolean, (default <code>TRUE</code>), whether the layer uses a bias vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kernel_initializer</code></td>
<td>
<p>Initializer for the <code>kernel</code> weights matrix, used for
the linear transformation of the inputs. Default: <code>glorot_uniform</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>recurrent_initializer</code></td>
<td>
<p>Initializer for the <code>recurrent_kernel</code> weights
matrix, used for the linear transformation of the recurrent state.
Default: <code>orthogonal</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bias_initializer</code></td>
<td>
<p>Initializer for the bias vector. Default: <code>zeros</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>unit_forget_bias</code></td>
<td>
<p>Boolean (default <code>TRUE</code>). If TRUE, add 1 to the bias of
the forget gate at initialization. Setting it to true will also force
<code>bias_initializer="zeros"</code>. This is recommended in <a href="https://proceedings.mlr.press/v37/jozefowicz15.pdf">Jozefowicz et al.</a></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kernel_regularizer</code></td>
<td>
<p>Regularizer function applied to the <code>kernel</code> weights
matrix. Default: <code>NULL</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>recurrent_regularizer</code></td>
<td>
<p>Regularizer function applied to
the <code>recurrent_kernel</code> weights matrix. Default: <code>NULL</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bias_regularizer</code></td>
<td>
<p>Regularizer function applied to the bias vector. Default:
<code>NULL</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kernel_constraint</code></td>
<td>
<p>Constraint function applied to the <code>kernel</code> weights
matrix. Default: <code>NULL</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>recurrent_constraint</code></td>
<td>
<p>Constraint function applied to the <code>recurrent_kernel</code>
weights matrix. Default: <code>NULL</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bias_constraint</code></td>
<td>
<p>Constraint function applied to the bias vector. Default:
<code>NULL</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dropout</code></td>
<td>
<p>Float between 0 and 1. Fraction of the units to drop for the linear
transformation of the inputs. Default: 0.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>recurrent_dropout</code></td>
<td>
<p>Float between 0 and 1. Fraction of the units to drop for
the linear transformation of the recurrent state. Default: 0.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>standard layer arguments.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>See <a href="https://www.tensorflow.org/guide/keras/rnn">the Keras RNN API guide</a>
for details about the usage of RNN API.
</p>
<p>This class processes one step within the whole time sequence input, whereas
<code>tf$keras$layer$LSTM</code> processes the whole sequence.
</p>
<p>For example:
</p>
<div class="sourceCode r"><pre>inputs &lt;- k_random_normal(c(32, 10, 8))
rnn &lt;- layer_rnn(cell = layer_lstm_cell(units = 4))
output &lt;- rnn(inputs)
dim(output) # (32, 4)

rnn &lt;- layer_rnn(cell = layer_lstm_cell(units = 4),
                 return_sequences = TRUE,
                 return_state = TRUE)
c(whole_seq_output, final_memory_state, final_carry_state) %&lt;-% rnn(inputs)

dim(whole_seq_output)   # (32, 10, 4)
dim(final_memory_state) # (32, 4)
dim(final_carry_state)  # (32, 4)
</pre></div>


<h3>See Also</h3>


<ul><li> <p><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell">https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell</a>
</p>
</li></ul>
<p>Other RNN cell layers: 
<code>layer_gru_cell()</code>,
<code>layer_simple_rnn_cell()</code>,
<code>layer_stacked_rnn_cells()</code>
</p>


</div>