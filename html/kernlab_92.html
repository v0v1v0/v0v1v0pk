<div class="container">

<table style="width: 100%;"><tr>
<td>inchol</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Incomplete Cholesky decomposition</h2>

<h3>Description</h3>

<p><code>inchol</code> computes the incomplete Cholesky decomposition
of the kernel matrix from a data matrix. 
</p>


<h3>Usage</h3>

<pre><code class="language-R">inchol(x, kernel="rbfdot", kpar=list(sigma=0.1), tol = 0.001, 
            maxiter = dim(x)[1], blocksize = 50, verbose = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>The data matrix indexed by row</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kernel</code></td>
<td>
<p>the kernel function used in training and predicting.
This parameter can be set to any function, of class <code>kernel</code>,
which computes the inner product in feature space between two
vector arguments. kernlab provides the most popular kernel functions
which can be used by setting the kernel parameter to the following
strings:
</p>

<ul>
<li> <p><code>rbfdot</code> Radial Basis kernel function "Gaussian"
</p>
</li>
<li> <p><code>polydot</code> Polynomial kernel function
</p>
</li>
<li> <p><code>vanilladot</code> Linear kernel function
</p>
</li>
<li> <p><code>tanhdot</code> Hyperbolic tangent kernel function
</p>
</li>
<li> <p><code>laplacedot</code> Laplacian kernel function
</p>
</li>
<li> <p><code>besseldot</code> Bessel kernel function
</p>
</li>
<li> <p><code>anovadot</code> ANOVA RBF kernel function
</p>
</li>
<li> <p><code>splinedot</code> Spline kernel 
</p>
</li>
</ul>
<p>The kernel parameter can also be set to a user defined function of
class kernel by passing the function name as an argument.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kpar</code></td>
<td>
<p>the list of hyper-parameters (kernel parameters).
This is a list which contains the parameters to be used with the
kernel function. Valid parameters for existing kernels are :
</p>

<ul>
<li> <p><code>sigma</code> inverse kernel width for the Radial Basis
kernel function "rbfdot" and the Laplacian kernel "laplacedot".
</p>
</li>
<li> <p><code>degree, scale, offset</code> for the Polynomial kernel "polydot"
</p>
</li>
<li> <p><code>scale, offset</code> for the Hyperbolic tangent kernel
function "tanhdot"
</p>
</li>
<li> <p><code>sigma, order, degree</code> for the Bessel kernel "besseldot". 
</p>
</li>
<li> <p><code>sigma, degree</code> for the ANOVA kernel "anovadot".
</p>
</li>
</ul>
<p>Hyper-parameters for user defined kernels can be passed through the
kpar parameter as well.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>
<p>algorithm stops when remaining pivots bring less accuracy
then <code>tol</code> (default: 0.001)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxiter</code></td>
<td>
<p>maximum number of iterations and columns in <code class="reqn">Z</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>blocksize</code></td>
<td>
<p>add this many columns to matrix per iteration</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>print info on algorithm convergence</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>An incomplete cholesky decomposition calculates
<code class="reqn">Z</code> where <code class="reqn">K= ZZ'</code> <code class="reqn">K</code> being the kernel matrix.
Since the rank of a kernel matrix is usually low, <code class="reqn">Z</code> tends to be smaller
then the complete kernel matrix. The decomposed matrix can be
used to create memory efficient kernel-based algorithms without the
need to compute and store a complete kernel matrix in memory.</p>


<h3>Value</h3>

<p>An S4 object of class "inchol" which is an extension of the class
"matrix". The object is the decomposed kernel matrix along with 
the slots :
</p>
<table>
<tr style="vertical-align: top;">
<td><code>pivots</code></td>
<td>
<p>Indices on which pivots where done</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>diagresidues</code></td>
<td>
<p>Residuals left on the diagonal</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxresiduals</code></td>
<td>
<p>Residuals picked for pivoting</p>
</td>
</tr>
</table>
<p>slots can be accessed either by <code>object@slot</code>
or by accessor functions with the same name (e.g., <code>pivots(object))</code></p>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou (based on Matlab code by 
S.V.N. (Vishy) Vishwanathan and Alex Smola)<br><a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>References</h3>

<p>Francis R. Bach, Michael I. Jordan<br><em>Kernel Independent Component Analysis</em><br>
Journal of Machine Learning Research  3, 1-48<br><a href="https://www.jmlr.org/papers/volume3/bach02a/bach02a.pdf">https://www.jmlr.org/papers/volume3/bach02a/bach02a.pdf</a>
</p>


<h3>See Also</h3>

<p><code>csi</code>, <code>inchol-class</code>, <code>chol</code></p>


<h3>Examples</h3>

<pre><code class="language-R">
data(iris)
datamatrix &lt;- as.matrix(iris[,-5])
# initialize kernel function
rbf &lt;- rbfdot(sigma=0.1)
rbf
Z &lt;- inchol(datamatrix,kernel=rbf)
dim(Z)
pivots(Z)
# calculate kernel matrix
K &lt;- crossprod(t(Z))
# difference between approximated and real kernel matrix
(K - kernelMatrix(kernel=rbf, datamatrix))[6,]

</code></pre>


</div>