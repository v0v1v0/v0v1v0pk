<div class="container">

<table style="width: 100%;"><tr>
<td>sample_sentences</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Random Text Generation</h2>

<h3>Description</h3>

<p>Sample sentences from a language model's probability distribution.
</p>


<h3>Usage</h3>

<pre><code class="language-R">sample_sentences(model, n, max_length, t = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>an object of class <code>language_model</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n</code></td>
<td>
<p>an integer. Number of sentences to sample.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_length</code></td>
<td>
<p>an integer. Maximum length of sampled sentences.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>t</code></td>
<td>
<p>a positive number. Sampling temperature (optional); see Details.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This function samples sentences according the prescribed language model's
probability distribution, with an optional temperature parameter.
The temperature transform of a probability distribution is defined by
<code>p(t) = exp(log(p) / t) / Z(t)</code> where <code>Z(t)</code> is the partition
function, fixed by the normalization condition <code>sum(p(t)) = 1</code>.
</p>
<p>Sampling is performed word by word, using the already sampled string
as context, starting from the Begin-Of-Sentence context (i.e. <code>N - 1</code>
BOS tokens). Sampling stops either when an End-Of-Sentence token is
encountered, or when the string exceeds <code>max_length</code>, in which case
a truncated output is returned.
</p>
<p>Some language models may give a non-zero probability to the the Unknown word
token, but this is never produced in text generated by
<code>sample_sentences()</code>: when randomly sampled, it is simply ignored.
</p>
<p>Finally, a word of caution on some special smoothers: <code>"sbo"</code> smoother
(Stupid Backoff), does not produce normalized continuation probabilities,
but rather continuation <em>scores</em>. Sampling is here performed by assuming
that Stupid Backoff scores are <em>proportional</em> to actual probabilities.
'ml' smoother (Maximum Likelihood) does not assign probabilities when the
k-gram count of the context is zero. When this happens, the next word is
chosen uniformly at random from the model's dictionary.
</p>


<h3>Value</h3>

<p>a character vector of length <code>n</code>. Random sentences generated
from the language model's distribution.
</p>


<h3>Author(s)</h3>

<p>Valerio Gherardi
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Sample sentences from 8-gram Kneser-Ney model trained on Shakespeare's
# "Much Ado About Nothing"



### Prepare the model and set seed
freqs &lt;- kgram_freqs(much_ado, 8, .tknz_sent = tknz_sent)
model &lt;- language_model(freqs, "kn", D = 0.75)
set.seed(840)

sample_sentences(model, n = 3, max_length = 10)

### Sampling at high temperature
sample_sentences(model, n = 3, max_length = 10, t = 100)

### Sampling at low temperature
sample_sentences(model, n = 3, max_length = 10, t = 0.01)


</code></pre>


</div>