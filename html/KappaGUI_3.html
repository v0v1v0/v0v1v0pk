<div class="container">

<table style="width: 100%;"><tr>
<td>kappaFleiss</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Calculates Fleiss' kappa between <code class="reqn">k</code> raters for all <code class="reqn">k</code>-uplets of columns in a given dataframe
</h2>

<h3>Description</h3>

<p>This function is based on the function 'kappam.fleiss' from the package 'irr', and simply adds the possibility of calculating several kappas at once.
</p>


<h3>Usage</h3>

<pre><code class="language-R">kappaFleiss(data, nb_raters=3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>dataframe with <code class="reqn">k \times p</code> columns, <code class="reqn">k</code> being the number of raters, and <code class="reqn">p</code> the number of traits. The first <code class="reqn">k</code> columns represent the scores attributed by the <code class="reqn">k</code> raters for the first trait; the next <code class="reqn">k</code> columns represent the scores attributed by the <code class="reqn">k</code> raters for the second trait; etc. The dataframe must contains a header, and each column must be labeled as follows: ‘VariableName_X’, where X is a unique character (letter or number) associated with each rater (cf. below for an example).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nb_raters</code></td>
<td>
<p>integer for the number of raters.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>For each trait, only complete cases are used for the calculation.
</p>


<h3>Value</h3>

<p>A dataframe with <code class="reqn">p</code> rows (one per trait) and two columns, giving respectively the kappa value for each trait, and the number of individuals used to calculate this value.
</p>


<h3>Author(s)</h3>

<p>Frédéric Santos, <a href="mailto:frederic.santos@u-bordeaux.fr">frederic.santos@u-bordeaux.fr</a>
</p>


<h3>References</h3>

<p>Cohen, J. (1960) A coefficient of agreement for nominal scales. <em>Educational and Psychological Measurement</em>, <b>20</b>, 37–46.
</p>
<p>Cohen, J. (1968) Weighted kappa: Nominal scale agreement with provision for scaled disagreement or partial credit. <em>Psychological Bulletin</em>, <b>70</b>, 213–220.
</p>


<h3>See Also</h3>

<p>irr::kappam.fleiss
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Here we create and display an artifical dataset,
# describing two traits coded by three raters:
scores &lt;- data.frame(
	Trait1_A = c(1,0,2,1,1,1,0,2,1,1),
	Trait1_B = c(1,2,0,1,2,1,0,1,2,1),
	Trait1_C = c(2,2,2,1,1,1,0,1,2,1),
	Trait2_A = c(1,4,5,2,3,5,1,2,3,4),
	Trait2_B = c(2,5,2,2,4,5,1,3,1,4),
	Trait2_C = c(2,4,3,2,4,5,2,2,3,4)
	)
scores

# Retrieve Fleiss' kappa for Trait1 and Trait2,
# to evaluate inter-rater agreement between raters A, B and C:
kappaFleiss(scores, nb_raters=3)
</code></pre>


</div>