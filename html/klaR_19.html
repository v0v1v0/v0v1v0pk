<div class="container">

<table style="width: 100%;"><tr>
<td>friedman.data</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Friedman's classification benchmark data</h2>

<h3>Description</h3>

<p>Function to generate 3-class classification benchmarking data
as introduced by J.H. Friedman (1989)</p>


<h3>Usage</h3>

<pre><code class="language-R">friedman.data(setting = 1, p = 6, samplesize = 40, asmatrix = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>setting</code></td>
<td>
<p>the problem setting (integer 1,2,...,6).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>p</code></td>
<td>
<p>number of variables (6, 10, 20 or 40).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>samplesize</code></td>
<td>
<p>sample size (number of observations, &gt;=6).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>asmatrix</code></td>
<td>
<p>if <code>TRUE</code>, results are returned as a matrix,
otherwise as a data frame (default).</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>When J.H. Friedman introduced the Regularized Discriminant Analysis
(<code>rda</code>) in 1989, he used artificially generated data
to test the procedure and to examine its performance in comparison to
Linear and Quadratic Discriminant Analysis
(see also <code>lda</code> and <code>qda</code>).
</p>
<p>6 different settings were considered to demonstrate potential strengths
and weaknesses of the new method:
</p>

<ol>
<li>
<p> equal spherical covariance matrices,
</p>
</li>
<li>
<p> unequal spherical covariance matrices,
</p>
</li>
<li>
<p> equal, highly ellipsoidal covariance matrices with mean
differences in low-variance subspace,
</p>
</li>
<li>
<p> equal, highly ellipsoidal covariance matrices with mean
differences in high-variance subspace,
</p>
</li>
<li>
<p> unequal, highly ellipsoidal covariance matrices with zero mean
differences and
</p>
</li>
<li>
<p> unequal, highly ellipsoidal covariance matrices with nonzero mean
differences.
</p>
</li>
</ol>
<p>For each of the 6 settings data was generated with 6, 10, 20 and 40
variables.
</p>
<p>Classification performance was then measured by repeatedly creating
training-datasets of 40 observations and estimating the
misclassification rates by test sets of 100 observations.
</p>
<p>The number of classes is always 3, class labels are assigned randomly
(with equal probabilities) to observations, so the contributions of
classes to the data differs from dataset to dataset. To make sure
covariances can be estimated at all, there are always at least two
observations from each class in a dataset.
</p>


<h3>Value</h3>

<p>Depending on <code>asmatrix</code> either a data frame or a matrix with
<code>samplesize</code> rows and <code>p+1</code> columns, the first column containing
the class labels, the remaining columns being the variables.
</p>


<h3>Author(s)</h3>

<p>Christian RÃ¶ver, <a href="mailto:roever@statistik.tu-dortmund.de">roever@statistik.tu-dortmund.de</a></p>


<h3>References</h3>

<p>Friedman, J.H. (1989): Regularized Discriminant Analysis.
In: <em>Journal of the American Statistical Association</em> 84, 165-175.</p>


<h3>See Also</h3>

<p><code>rda</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Reproduce the 1st setting with 6 variables.
# Error rate should be somewhat near 9 percent.
training &lt;- friedman.data(1, 6, 40)
x &lt;- rda(class ~ ., data = training, gamma = 0.74, lambda = 0.77)
test &lt;- friedman.data(1, 6, 100)
y &lt;- predict(x, test[,-1])
errormatrix(test[,1], y$class)
</code></pre>


</div>