<div class="container">

<table style="width: 100%;"><tr>
<td>bidirectional</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>layer_bidirectional</h2>

<h3>Description</h3>

<p><code>bidirectional()</code> is an alias for <code>layer_bidirectional()</code>.
See <code style="white-space: pre;">⁠?⁠</code><code>layer_bidirectional()</code> for the full documentation.
</p>


<h3>Usage</h3>

<pre><code class="language-R">bidirectional(
  object,
  layer,
  merge_mode = "concat",
  weights = NULL,
  backward_layer = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>Object to compose the layer with. A tensor, array, or sequential model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>layer</code></td>
<td>
<p><code>RNN</code> instance, such as
<code>layer_lstm()</code> or <code>layer_gru()</code>.
It could also be a <code>Layer()</code> instance
that meets the following criteria:
</p>

<ol>
<li>
<p> Be a sequence-processing layer (accepts 3D+ inputs).
</p>
</li>
<li>
<p> Have a <code>go_backwards</code>, <code>return_sequences</code> and <code>return_state</code>
attribute (with the same semantics as for the <code>RNN</code> class).
</p>
</li>
<li>
<p> Have an <code>input_spec</code> attribute.
</p>
</li>
<li>
<p> Implement serialization via <code>get_config()</code> and <code>from_config()</code>.
Note that the recommended way to create new RNN layers is to write a
custom RNN cell and use it with <code>layer_rnn()</code>, instead of
subclassing with <code>Layer()</code> directly.
When <code>return_sequences</code> is <code>TRUE</code>, the output of the masked
timestep will be zero regardless of the layer's original
<code>zero_output_for_mask</code> value.
</p>
</li>
</ol>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>merge_mode</code></td>
<td>
<p>Mode by which outputs of the forward and backward RNNs
will be combined. One of <code style="white-space: pre;">⁠{"sum", "mul", "concat", "ave", NULL}⁠</code>.
If <code>NULL</code>, the outputs will not be combined,
they will be returned as a list. Defaults to <code>"concat"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>see description</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>backward_layer</code></td>
<td>
<p>Optional <code>RNN</code>,
or <code>Layer()</code> instance to be used to handle
backwards input processing.
If <code>backward_layer</code> is not provided, the layer instance passed
as the <code>layer</code> argument will be used to generate the backward layer
automatically.
Note that the provided <code>backward_layer</code> layer should have properties
matching those of the <code>layer</code> argument, in particular
it should have the same values for <code>stateful</code>, <code>return_states</code>,
<code>return_sequences</code>, etc. In addition, <code>backward_layer</code>
and <code>layer</code> should have different <code>go_backwards</code> argument values.
A <code>ValueError</code> will be raised if these requirements are not met.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>For forward/backward compatability.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>The return value depends on the value provided for the first argument.
If  <code>object</code> is:
</p>

<ul>
<li>
<p> a <code>keras_model_sequential()</code>, then the layer is added to the sequential model
(which is modified in place). To enable piping, the sequential model is also
returned, invisibly.
</p>
</li>
<li>
<p> a <code>keras_input()</code>, then the output tensor from calling <code>layer(input)</code> is returned.
</p>
</li>
<li> <p><code>NULL</code> or missing, then a <code>Layer</code> instance is returned.
</p>
</li>
</ul>
</div>