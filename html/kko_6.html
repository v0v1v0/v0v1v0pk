<div class="container">

<table style="width: 100%;"><tr>
<td>rk_tune</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>tune random feature number for KKO.</h2>

<h3>Description</h3>

<p>The function applys KKO with different random feature numbers to tune the optimal number.
</p>


<h3>Usage</h3>

<pre><code class="language-R">rk_tune(
  X,
  y,
  X_k,
  rfn_range,
  n_stb,
  cv_folds,
  frac_stb = 1/2,
  nCores_para = 1,
  rkernel = "laplacian",
  rk_scale = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>design matrix of additive model; rows are observations and columns are variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>response of addtive model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X_k</code></td>
<td>
<p>knockoffs matrix of design; the same size as X.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rfn_range</code></td>
<td>
<p>a vector of random feature expansion numbers to be tuned.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_stb</code></td>
<td>
<p>number of subsampling in KKO.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cv_folds</code></td>
<td>
<p>the folds of cross-validation for tuning group lasso.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>frac_stb</code></td>
<td>
<p>fraction of subsample.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nCores_para</code></td>
<td>
<p>number of cores for parallelizing subsampling.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rkernel</code></td>
<td>
<p>kernel choices. Default is "laplacian". Other choices are "cauchy" and "gaussian".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rk_scale</code></td>
<td>
<p>scaling parameter of sampling distribution for random feature expansion. For gaussian kernel, it is standard deviation of gaussian sampling distribution.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>a list of tuning results.
</p>

<table>
<tr>
<td style="text-align: left;">
<code>rfn_tune</code>  </td>
<td style="text-align: left;">  tuned optimal random feature number.  </td>
</tr>
<tr>
<td style="text-align: left;">
<code>rfn_range</code>  </td>
<td style="text-align: left;"> a vector of random feature expansion numbers to be tuned. </td>
</tr>
<tr>
<td style="text-align: left;">
<code>scores</code> </td>
<td style="text-align: left;"> scores of random feature numbers. rfn_tune has the maximal score. </td>
</tr>
<tr>
<td style="text-align: left;">
<code>Pi_list</code> </td>
<td style="text-align: left;"> a list of subsample selection results for each random feature number. </td>
</tr>
<tr>
<td style="text-align: left;">
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Xiaowu Dai, Xiang Lyu, Lexin Li
</p>


<h3>Examples</h3>

<pre><code class="language-R">library(knockoff)
p=5 # number of predictors
sig_mag=100 # signal strength
n= 100 # sample size
rkernel="laplacian" # kernel choice
s=2  # sparsity, number of nonzero component functions
rk_scale=1  # scaling paramtere of kernel
rfn_range= c(2,3,4)  # number of random features
cv_folds=15  # folds of cross-validation in group lasso
n_stb=10 # number of subsampling
frac_stb=1/2 # fraction of subsample
nCores_para=2 # number of cores for parallelization
X=matrix(rnorm(n*p),n,p)%*%chol(toeplitz(0.3^(0:(p-1))))   # generate design
X_k = create.second_order(X) # generate knockoff
reg_coef=c(rep(1,s),rep(0,p-s))  # regression coefficient
reg_coef=reg_coef*(2*(rnorm(p)&gt;0)-1)*sig_mag
y=X%*% reg_coef + rnorm(n) # response

rk_tune(X,y,X_k,rfn_range,n_stb,cv_folds,frac_stb,nCores_para,rkernel,rk_scale)


</code></pre>


</div>