<div class="container">

<table style="width: 100%;"><tr>
<td>adapt</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Fits the state of the preprocessing layer to the data being passed</h2>

<h3>Description</h3>

<p>Fits the state of the preprocessing layer to the data being passed
</p>


<h3>Usage</h3>

<pre><code class="language-R">adapt(object, data, ..., batch_size = NULL, steps = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>Preprocessing layer object</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>The data to train on. It can be passed either as a
<code>tf.data.Dataset</code> or as an R array.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Used for forwards and backwards compatibility. Passed on to the underlying method.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>batch_size</code></td>
<td>
<p>Integer or <code>NULL</code>. Number of asamples per state update. If
unspecified, <code>batch_size</code> will default to <code>32</code>. Do not specify the
batch_size if your data is in the form of a TF Dataset or a generator
(since they generate batches).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>steps</code></td>
<td>
<p>Integer or <code>NULL</code>. Total number of steps (batches of samples)
When training with input tensors such as TensorFlow data tensors, the
default <code>NULL</code> is equal to the number of samples in your dataset divided by
the batch size, or <code>1</code> if that cannot be determined. If x is a
<code>tf.data.Dataset</code>, and <code>steps</code> is <code>NULL</code>, the epoch will run until the
input dataset is exhausted. When passing an infinitely repeating dataset,
you must specify the steps argument. This argument is not supported with
array inputs.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>After calling <code>adapt</code> on a layer, a preprocessing layer's state will not
update during training. In order to make preprocessing layers efficient in
any distribution context, they are kept constant with respect to any
compiled <code>tf.Graph</code>s that call the layer. This does not affect the layer use
when adapting each layer only once, but if you adapt a layer multiple times
you will need to take care to re-compile any compiled functions as follows:
</p>

<ul>
<li>
<p> If you are adding a preprocessing layer to a keras model, you need to
call <code>compile(model)</code> after each subsequent call to <code>adapt()</code>.
</p>
</li>
<li>
<p> If you are calling a preprocessing layer inside <code>tfdatasets::dataset_map()</code>,
you should call <code>dataset_map()</code> again on the input <code>Dataset</code> after each
<code>adapt()</code>.
</p>
</li>
<li>
<p> If you are using a <code>tensorflow::tf_function()</code> directly which calls a preprocessing
layer, you need to call <code>tf_function()</code> again on your callable after
each subsequent call to <code>adapt()</code>.
</p>
</li>
</ul>
<p><code>keras_model()</code> example with multiple adapts:
</p>
<div class="sourceCode r"><pre>layer &lt;- layer_normalization(axis = NULL)
adapt(layer, c(0, 2))
model &lt;- keras_model_sequential() |&gt; layer()
predict(model, c(0, 1, 2), verbose = FALSE) # [1] -1  0  1
</pre></div>
<div class="sourceCode"><pre>## [1] -1  0  1

</pre></div>
<div class="sourceCode r"><pre>adapt(layer, c(-1, 1))
compile(model)  # This is needed to re-compile model.predict!
predict(model, c(0, 1, 2), verbose = FALSE) # [1] 0 1 2
</pre></div>
<div class="sourceCode"><pre>## [1] 0 1 2

</pre></div>
<p><code>tfdatasets</code> example with multiple adapts:
</p>
<div class="sourceCode r"><pre>layer &lt;- layer_normalization(axis = NULL)
adapt(layer, c(0, 2))
input_ds &lt;- tfdatasets::range_dataset(0, 3)
normalized_ds &lt;- input_ds |&gt;
  tfdatasets::dataset_map(layer)
str(tfdatasets::iterate(normalized_ds))
</pre></div>
<div class="sourceCode"><pre>## List of 3
##  $ :&lt;tf.Tensor: shape=(1), dtype=float32, numpy=array([-1.], dtype=float32)&gt;
##  $ :&lt;tf.Tensor: shape=(1), dtype=float32, numpy=array([0.], dtype=float32)&gt;
##  $ :&lt;tf.Tensor: shape=(1), dtype=float32, numpy=array([1.], dtype=float32)&gt;

</pre></div>
<div class="sourceCode r"><pre>adapt(layer, c(-1, 1))
normalized_ds &lt;- input_ds |&gt;
  tfdatasets::dataset_map(layer) # Re-map over the input dataset.

normalized_ds |&gt;
  tfdatasets::as_array_iterator() |&gt;
  tfdatasets::iterate(simplify = FALSE) |&gt;
  str()
</pre></div>
<div class="sourceCode"><pre>## List of 3
##  $ : num [1(1d)] 0
##  $ : num [1(1d)] 1
##  $ : num [1(1d)] 2

</pre></div>


<h3>Value</h3>

<p>Returns <code>object</code>, invisibly.
</p>


</div>