<div class="container">

<table style="width: 100%;"><tr>
<td>KFOCI</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Kernel Feature Ordering by Conditional Independence</h2>

<h3>Description</h3>

<p>Variable selection with KPC using directed K-NN graph or minimum spanning tree (MST)
</p>


<h3>Usage</h3>

<pre><code class="language-R">KFOCI(
  Y,
  X,
  k = kernlab::rbfdot(1/(2 * stats::median(stats::dist(Y))^2)),
  Knn = min(ceiling(NROW(Y)/20), 20),
  num_features = NULL,
  stop = TRUE,
  numCores = parallel::detectCores(),
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>
<p>a matrix of responses (n by dy)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>a matrix of predictors (n by dx)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>a function <code class="reqn">k(y, y')</code> of class <code>kernel</code>. It can be the kernel implemented in <code>kernlab</code> e.g., Gaussian kernel: <code>rbfdot(sigma = 1)</code>, linear kernel: <code>vanilladot()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Knn</code></td>
<td>
<p>a positive integer indicating the number of nearest neighbor; or "MST". The suggested choice of Knn is 0.05n for samples up to a few hundred observations. For large n, the suggested Knn is sublinear in n. That is, it may grow slower than any linear function of n. The computing time is approximately linear in Knn. A smaller Knn takes less time.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num_features</code></td>
<td>
<p>the number of variables to be selected, cannot be larger than dx. The default value is NULL and in that
case it will be set equal to dx. If <code>stop == TRUE</code> (see below), then num_features is the maximal number of variables to be selected.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stop</code></td>
<td>
<p>If <code>stop == TRUE</code>, then the automatic stopping criterion (stops at the first instance of negative Tn, as mentioned in the paper) will be implemented and continued till <code>num_features</code> many variables are selected. If <code>stop == FALSE</code> then exactly <code>num_features</code> many variables are selected.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>numCores</code></td>
<td>
<p>number of cores that are going to be used for parallelizing the process.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>whether to print each selected variables during the forward stepwise algorithm</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>A stepwise forward selection of variables using KPC. At each step it selects the <code class="reqn">X_j</code> that maximizes
<code class="reqn">\hat{\rho^2}(Y,X_j |</code>selected <code class="reqn">X_i)</code>.
It is suggested to normalize the predictors before applying KFOCI.
Euclidean distance is used for computing the K-NN graph and the MST.
</p>


<h3>Value</h3>

<p>The algorithm returns a vector of the indices from 1,...,dx of the selected variables in the same order that they were selected. The variables at the front are expected to be more informative in predicting Y.
</p>


<h3>See Also</h3>

<p><code>KPCgraph</code>, <code>KPCRKHS</code>, <code>KPCRKHS_VS</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">n = 200
p = 10
X = matrix(rnorm(n * p), ncol = p)
Y = X[, 1] * X[, 2] + sin(X[, 1] * X[, 3])
KFOCI(Y, X, kernlab::rbfdot(1), Knn=1, numCores=1)
# 1 2 3
</code></pre>


</div>