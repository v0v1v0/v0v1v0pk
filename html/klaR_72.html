<div class="container">

<table style="width: 100%;"><tr>
<td>rda</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Regularized Discriminant Analysis (RDA)</h2>

<h3>Description</h3>

<p>Builds a classification rule using regularized group covariance 
matrices that are supposed to be more robust against 
multicollinearity in the data.
</p>


<h3>Usage</h3>

<pre><code class="language-R">rda(x, ...)

## Default S3 method:
rda(x, grouping = NULL, prior = NULL, gamma = NA, 
    lambda = NA, regularization = c(gamma = gamma, lambda = lambda), 
    crossval = TRUE, fold = 10, train.fraction = 0.5, 
    estimate.error = TRUE, output = FALSE, startsimplex = NULL, 
    max.iter = 100, trafo = TRUE, simAnn = FALSE, schedule = 2, 
    T.start = 0.1, halflife = 50, zero.temp = 0.01, alpha = 2, 
    K = 100, ...)
## S3 method for class 'formula'
rda(formula, data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>Matrix or data frame containing the explanatory variables 
(required, if <code>formula</code> is not given).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>Formula of the form ‘<code>groups ~ x1 + x2 + ...</code>’.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>A data frame (or matrix) containing the explanatory 
variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>grouping</code></td>
<td>
<p>(Optional) a vector specifying the class for 
each observation; if not specified, the first column of 
‘<code>data</code>’ is taken.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prior</code></td>
<td>
<p>(Optional) prior probabilities for the classes. 
Default: proportional to training sample sizes. 
“<code>prior=1</code>” indicates equally likely classes.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma, lambda, regularization</code></td>
<td>

<p>One or both of the rda-parameters may be fixed manually. 
Unspecified parameters are determined by minimizing the 
estimated error rate (see below).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>crossval</code></td>
<td>
<p>Logical. If <code>TRUE</code>, in the optimization 
step the error rate is estimated by Cross-Validation, 
otherwise by drawing several training- and test-samples.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fold</code></td>
<td>
<p>The number of Cross-Validation- or Bootstrap-samples
to be drawn.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>train.fraction</code></td>
<td>
<p>In case of Bootstrapping: the fraction of 
the data to be used for training in each Bootstrap-sample; 
the remainder is used to estimate the misclassification rate.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>estimate.error</code></td>
<td>
<p>Logical. If <code>TRUE</code>, the apparent 
error rate for the final parameter set is estimated.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>output</code></td>
<td>
<p>Logical flag to indicate whether text output 
during computation is desired.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>startsimplex</code></td>
<td>
<p>(Optional) a starting simplex for the 
Nelder-Mead-minimization.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max.iter</code></td>
<td>
<p>Maximum number of iterations for Nelder-Mead.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trafo</code></td>
<td>
<p>Logical; indicates whether minimization is carrried 
out using transformed parameters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>simAnn</code></td>
<td>
<p>Logical; indicates whether Simulated Annealing 
shall be used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>schedule</code></td>
<td>
<p>Annealing schedule 1 or 2 (exponential or polynomial).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>T.start</code></td>
<td>
<p>Starting temperature for Simulated Annealing.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>halflife</code></td>
<td>
<p>Number of iterations until temperature is reduced to a half  (schedule 1).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>zero.temp</code></td>
<td>
<p>Temperature at which it is set to zero  (schedule 1).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>Power of temperature reduction (linear, quadratic, cubic,...)  (schedule 2).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>K</code></td>
<td>
<p>Number of iterations until temperature = 0  (schedule 2).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>currently unused</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>J.H. Friedman (see references below) suggested a method to fix 
almost singular covariance matrices in discriminant analysis. 
Basically, individual covariances as in QDA are used, but 
depending on two parameters (<code class="reqn">\gamma</code> and 
<code class="reqn">\lambda</code>), these can be shifted towards a 
diagonal matrix and/or the pooled covariance 
matrix. For (<code class="reqn">\gamma=0</code>, <code class="reqn">\lambda=0</code>) it equals QDA, 
for (<code class="reqn">\gamma=0</code>, <code class="reqn">\lambda=1</code>) it equals LDA.
</p>
<p>You may fix these parameters at certain values or leave it to 
the function to try to find “optimal” values. If one 
parameter is given, the other one is determined using the 
R-function ‘<code>optimize</code>’. If no parameter is 
given, both are determined numerically by a 
Nelder-Mead-(Simplex-)algorithm with the option of using 
Simulated Annealing.
The goal function to be minimized is the (estimated) 
misclassification rate; the misclassification rate is estimated 
either by Cross-Validation or by repeatedly dividing the data 
into training- and test-sets (Boostrapping).
</p>
<p><em>Warning</em>: If these sets are small, optimization is expected 
to produce almost random results. We recommend to adjust the 
parameters manually in such a case.
In all other cases it is recommended to run the optimization 
several times in order to see whether stable results are gained.
</p>
<p>Since the Nelder-Mead-algorithm is actually intended for 
<em>continuous</em> functions while the observed error rate 
by its nature is <em>discrete</em>, a greater number of 
Boostrap-samples might improve the optimization by increasing 
the smoothness of the response surface (and, of course, by 
reducing variance and bias). 
If a set of parameters leads to singular covariance 
matrices, a penalty term is added to the misclassification rate 
which will hopefully help to maneuver back out of singularity
(so do not worry about error rates greater than one during 
optimization).
</p>


<h3>Value</h3>

<p>A list of class <code>rda</code> containing the following 
components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>call</code></td>
<td>
<p>The (matched) function call.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>regularization</code></td>
<td>
<p>vector containing the two regularization 
parameters (gamma, lambda)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>classes</code></td>
<td>
<p>the names of the classes</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prior</code></td>
<td>
<p>the prior probabilities for the classes</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>error.rate</code></td>
<td>
<p>apparent error rate (if computation 
was not suppressed), and, if any optimization took place, the
final (cross-validated or bootstrapped) error rate estimate as 
well.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>means</code></td>
<td>
<p>Group means.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>covariances</code></td>
<td>
<p>Array of group covariances.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>covpooled</code></td>
<td>
<p>Pooled covariance.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>converged</code></td>
<td>
<p>(Logical) indicator of convergence (only for 
Nelder-Mead).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iter</code></td>
<td>
<p>Number of iterations actually performed (only for 
Nelder-Mead).</p>
</td>
</tr>
</table>
<h3>More details</h3>

<p>The explicit defintion of <code class="reqn">\gamma</code>,
<code class="reqn">\lambda</code> and the resulting covariance estimates
is as follows:
</p>
<p>The pooled covariance estimate <code class="reqn">\hat{\Sigma}</code> is 
given as well as the individual covariance estimates 
<code class="reqn">\hat{\Sigma}_k</code> for each group.
</p>
<p>First, using <code class="reqn">\lambda</code>, a convex combination of 
these two is computed:
</p>
<p style="text-align: center;"><code class="reqn">\hat{\Sigma}_k (\lambda) := (1-\lambda) \hat{\Sigma}_k + \lambda \hat{\Sigma}.</code>
</p>

<p>Then, another convex combination is constructed using the 
above estimate and a (scaled) identity matrix:
</p>
<p style="text-align: center;"><code class="reqn">\hat{\Sigma}_k (\lambda,\gamma) = (1-\gamma)\hat{\Sigma}_k(\lambda)+
\gamma\frac{1}{d}\mathrm{tr}[\hat{\Sigma}_k(\lambda)]\mathrm{I}.</code>
</p>

<p>The factor 
<code class="reqn">\frac{1}{d}\mathrm{tr}[\hat{\Sigma}_k(\lambda)]</code>
in front of the identity matrix I is the mean of the diagonal 
elements of 
<code class="reqn">\hat{\Sigma}_k(\lambda)</code>, so it is 
the mean variance of all <code class="reqn">d</code> variables assuming the group 
covariance <code class="reqn">\hat{\Sigma}_k(\lambda)</code>.
</p>
<p>For the four extremes of (<code class="reqn">\gamma</code>,<code class="reqn">\lambda</code>) 
the covariance structure reduces to special cases:
</p>

<ul>
<li>
<p> (<code class="reqn">\gamma=0</code>, <code class="reqn">\lambda=0</code>): 
QDA - individual covariance for each group.
</p>
</li>
<li>
<p> (<code class="reqn">\gamma=0</code>, <code class="reqn">\lambda=1</code>): 
LDA - a common covariance matrix.
</p>
</li>
<li>
<p> (<code class="reqn">\gamma=1</code>, <code class="reqn">\lambda=0</code>): 
Conditional independent variables - similar to Naive Bayes,
but variable variances within group (main diagonal elements) 
are equal.
</p>
</li>
<li>
<p> (<code class="reqn">\gamma=1</code>, <code class="reqn">\lambda=1</code>):
Classification using euclidean distance - as in previous case, 
but variances are the same for all groups. Objects are assigned 
to group with nearest mean.
</p>
</li>
</ul>
<h3>Author(s)</h3>

<p>Christian Röver, <a href="mailto:roever@statistik.tu-dortmund.de">roever@statistik.tu-dortmund.de</a></p>


<h3>References</h3>

<p>Friedman, J.H. (1989): Regularized Discriminant Analysis.
In: <em>Journal of the American Statistical Association</em> 84, 
165-175.
</p>
<p>Press, W.H., Flannery, B.P., Teukolsky, S.A., Vetterling, W.T. (1992): 
<em>Numerical Recipes in C</em>.  Cambridge: Cambridge University Press.
</p>


<h3>See Also</h3>

<p><code>predict.rda</code>,
<code>lda</code>, <code>qda</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">data(iris)
x &lt;- rda(Species ~ ., data = iris, gamma = 0.05, lambda = 0.2)
predict(x, iris)
</code></pre>


</div>