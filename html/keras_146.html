<div class="container">

<table style="width: 100%;"><tr>
<td>initializer_lecun_normal</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>LeCun normal initializer.</h2>

<h3>Description</h3>

<p>It draws samples from a truncated normal distribution centered on 0 with
<code>stddev &lt;- sqrt(1 / fan_in)</code> where <code>fan_in</code> is the number of input units in
the weight tensor..
</p>


<h3>Usage</h3>

<pre><code class="language-R">initializer_lecun_normal(seed = NULL)
</code></pre>


<h3>Arguments</h3>

<table><tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p>A Python integer. Used to seed the random generator.</p>
</td>
</tr></table>
<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1706.02515">Self-Normalizing Neural Networks</a>
</p>
</li>
<li>
<p> Efficient Backprop, <cite>LeCun, Yann et al. 1998</cite>
</p>
</li>
</ul>
<h3>See Also</h3>

<p>Other initializers: 
<code>initializer_constant()</code>,
<code>initializer_glorot_normal()</code>,
<code>initializer_glorot_uniform()</code>,
<code>initializer_he_normal()</code>,
<code>initializer_he_uniform()</code>,
<code>initializer_identity()</code>,
<code>initializer_lecun_uniform()</code>,
<code>initializer_ones()</code>,
<code>initializer_orthogonal()</code>,
<code>initializer_random_normal()</code>,
<code>initializer_random_uniform()</code>,
<code>initializer_truncated_normal()</code>,
<code>initializer_variance_scaling()</code>,
<code>initializer_zeros()</code>
</p>


</div>