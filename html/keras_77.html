<div class="container">

<table style="width: 100%;"><tr>
<td>constraints</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Weight constraints</h2>

<h3>Description</h3>

<p>Functions that impose constraints on weight values.
</p>


<h3>Usage</h3>

<pre><code class="language-R">constraint_maxnorm(max_value = 2, axis = 0)

constraint_nonneg()

constraint_unitnorm(axis = 0)

constraint_minmaxnorm(min_value = 0, max_value = 1, rate = 1, axis = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>max_value</code></td>
<td>
<p>The maximum norm for the incoming weights.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>axis</code></td>
<td>
<p>The axis along which to calculate weight norms. For instance, in
a dense layer the weight matrix has shape <code style="white-space: pre;">⁠input_dim, output_dim⁠</code>, set
<code>axis</code> to <code>0</code> to constrain each weight vector of length <code style="white-space: pre;">⁠input_dim,⁠</code>. In a
convolution 2D layer with <code>dim_ordering="tf"</code>, the weight tensor has shape
<code style="white-space: pre;">⁠rows, cols, input_depth, output_depth⁠</code>, set <code>axis</code> to <code>c(0, 1, 2)</code> to
constrain the weights of each filter tensor of size <code style="white-space: pre;">⁠rows, cols, input_depth⁠</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min_value</code></td>
<td>
<p>The minimum norm for the incoming weights.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rate</code></td>
<td>
<p>The rate for enforcing the constraint: weights will be rescaled to
yield (1 - rate) * norm + rate * norm.clip(low, high). Effectively, this
means that rate=1.0 stands for strict enforcement of the constraint, while
rate&lt;1.0 means that weights will be rescaled at each step to slowly move
towards a value inside the desired interval.</p>
</td>
</tr>
</table>
<h3>Details</h3>


<ul>
<li> <p><code>constraint_maxnorm()</code> constrains the weights incident to each
hidden unit to have a norm less than or equal to a desired value.
</p>
</li>
<li> <p><code>constraint_nonneg()</code> constraints the weights to be non-negative
</p>
</li>
<li> <p><code>constraint_unitnorm()</code> constrains the weights incident to each hidden
unit to have unit norm.
</p>
</li>
<li> <p><code>constraint_minmaxnorm()</code> constrains the weights incident to each
hidden unit to have the norm between a lower bound and an upper bound.
</p>
</li>
</ul>
<h3>Custom constraints</h3>

<p>You can implement your own constraint functions in R. A custom
constraint is an R function that takes weights (<code>w</code>) as input
and returns modified weights. Note that keras <code>backend()</code> tensor
functions (e.g. <code>k_greater_equal()</code>) should be used in the
implementation of custom constraints. For example:
</p>
<div class="sourceCode r"><pre>nonneg_constraint &lt;- function(w) {
  w * k_cast(k_greater_equal(w, 0), k_floatx())
}

layer_dense(units = 32, input_shape = c(784),
            kernel_constraint = nonneg_constraint)
</pre></div>
<p>Note that models which use custom constraints cannot be serialized using
<code>save_model_hdf5()</code>. Rather, the weights of the model should be saved
and restored using <code>save_model_weights_hdf5()</code>.
</p>


<h3>See Also</h3>

<p><a href="https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf">Dropout: A Simple Way to Prevent Neural Networks from Overfitting Srivastava, Hinton, et al. 2014</a>
</p>
<p>KerasConstraint
</p>


</div>