<div class="container">

<table style="width: 100%;"><tr>
<td>metric_binary_focal_crossentropy</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Computes the binary focal crossentropy loss.</h2>

<h3>Description</h3>

<p>According to <a href="https://arxiv.org/pdf/1708.02002">Lin et al., 2018</a>, it
helps to apply a focal factor to down-weight easy examples and focus more on
hard examples. By default, the focal tensor is computed as follows:
</p>
<p><code>focal_factor = (1 - output)^gamma</code> for class 1
<code>focal_factor = output^gamma</code> for class 0
where <code>gamma</code> is a focusing parameter. When <code>gamma</code> = 0, there is no focal
effect on the binary crossentropy loss.
</p>
<p>If <code>apply_class_balancing == TRUE</code>, this function also takes into account a
weight balancing factor for the binary classes 0 and 1 as follows:
</p>
<p><code>weight = alpha</code> for class 1 (<code>target == 1</code>)
<code>weight = 1 - alpha</code> for class 0
where <code>alpha</code> is a float in the range of <code style="white-space: pre;">⁠[0, 1]⁠</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">metric_binary_focal_crossentropy(
  y_true,
  y_pred,
  apply_class_balancing = FALSE,
  alpha = 0.25,
  gamma = 2,
  from_logits = FALSE,
  label_smoothing = 0,
  axis = -1L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y_true</code></td>
<td>
<p>Ground truth values, of shape <code style="white-space: pre;">⁠(batch_size, d0, .. dN)⁠</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y_pred</code></td>
<td>
<p>The predicted values, of shape <code style="white-space: pre;">⁠(batch_size, d0, .. dN)⁠</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>apply_class_balancing</code></td>
<td>
<p>A bool, whether to apply weight balancing on the
binary classes 0 and 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>A weight balancing factor for class 1, default is <code>0.25</code> as
mentioned in the reference. The weight for class 0 is <code>1.0 - alpha</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma</code></td>
<td>
<p>A focusing parameter, default is <code>2.0</code> as mentioned in the
reference.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>from_logits</code></td>
<td>
<p>Whether <code>y_pred</code> is expected to be a logits tensor. By
default, we assume that <code>y_pred</code> encodes a probability distribution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>label_smoothing</code></td>
<td>
<p>Float in <code style="white-space: pre;">⁠[0, 1]⁠</code>. If &gt; <code>0</code> then smooth the labels by
squeezing them towards 0.5, that is,
using <code>1. - 0.5 * label_smoothing</code> for the target class
and <code>0.5 * label_smoothing</code> for the non-target class.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>axis</code></td>
<td>
<p>The axis along which the mean is computed. Defaults to <code>-1</code>.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>Binary focal crossentropy loss value
with shape = <code style="white-space: pre;">⁠[batch_size, d0, .. dN-1]⁠</code>.
</p>


<h3>Examples</h3>

<div class="sourceCode r"><pre>y_true &lt;- rbind(c(0, 1), c(0, 0))
y_pred &lt;- rbind(c(0.6, 0.4), c(0.4, 0.6))
loss &lt;- loss_binary_focal_crossentropy(y_true, y_pred, gamma=2)
loss
</pre></div>
<div class="sourceCode"><pre>## tf.Tensor([0.32986466 0.20579838], shape=(2), dtype=float64)

</pre></div>


<h3>See Also</h3>

<p>Other losses: <br><code>Loss()</code> <br><code>loss_binary_crossentropy()</code> <br><code>loss_binary_focal_crossentropy()</code> <br><code>loss_categorical_crossentropy()</code> <br><code>loss_categorical_focal_crossentropy()</code> <br><code>loss_categorical_hinge()</code> <br><code>loss_cosine_similarity()</code> <br><code>loss_ctc()</code> <br><code>loss_dice()</code> <br><code>loss_hinge()</code> <br><code>loss_huber()</code> <br><code>loss_kl_divergence()</code> <br><code>loss_log_cosh()</code> <br><code>loss_mean_absolute_error()</code> <br><code>loss_mean_absolute_percentage_error()</code> <br><code>loss_mean_squared_error()</code> <br><code>loss_mean_squared_logarithmic_error()</code> <br><code>loss_poisson()</code> <br><code>loss_sparse_categorical_crossentropy()</code> <br><code>loss_squared_hinge()</code> <br><code>loss_tversky()</code> <br><code>metric_binary_crossentropy()</code> <br><code>metric_categorical_crossentropy()</code> <br><code>metric_categorical_focal_crossentropy()</code> <br><code>metric_categorical_hinge()</code> <br><code>metric_hinge()</code> <br><code>metric_huber()</code> <br><code>metric_kl_divergence()</code> <br><code>metric_log_cosh()</code> <br><code>metric_mean_absolute_error()</code> <br><code>metric_mean_absolute_percentage_error()</code> <br><code>metric_mean_squared_error()</code> <br><code>metric_mean_squared_logarithmic_error()</code> <br><code>metric_poisson()</code> <br><code>metric_sparse_categorical_crossentropy()</code> <br><code>metric_squared_hinge()</code> <br></p>
<p>Other metrics: <br><code>Metric()</code> <br><code>custom_metric()</code> <br><code>metric_auc()</code> <br><code>metric_binary_accuracy()</code> <br><code>metric_binary_crossentropy()</code> <br><code>metric_binary_iou()</code> <br><code>metric_categorical_accuracy()</code> <br><code>metric_categorical_crossentropy()</code> <br><code>metric_categorical_focal_crossentropy()</code> <br><code>metric_categorical_hinge()</code> <br><code>metric_cosine_similarity()</code> <br><code>metric_f1_score()</code> <br><code>metric_false_negatives()</code> <br><code>metric_false_positives()</code> <br><code>metric_fbeta_score()</code> <br><code>metric_hinge()</code> <br><code>metric_huber()</code> <br><code>metric_iou()</code> <br><code>metric_kl_divergence()</code> <br><code>metric_log_cosh()</code> <br><code>metric_log_cosh_error()</code> <br><code>metric_mean()</code> <br><code>metric_mean_absolute_error()</code> <br><code>metric_mean_absolute_percentage_error()</code> <br><code>metric_mean_iou()</code> <br><code>metric_mean_squared_error()</code> <br><code>metric_mean_squared_logarithmic_error()</code> <br><code>metric_mean_wrapper()</code> <br><code>metric_one_hot_iou()</code> <br><code>metric_one_hot_mean_iou()</code> <br><code>metric_poisson()</code> <br><code>metric_precision()</code> <br><code>metric_precision_at_recall()</code> <br><code>metric_r2_score()</code> <br><code>metric_recall()</code> <br><code>metric_recall_at_precision()</code> <br><code>metric_root_mean_squared_error()</code> <br><code>metric_sensitivity_at_specificity()</code> <br><code>metric_sparse_categorical_accuracy()</code> <br><code>metric_sparse_categorical_crossentropy()</code> <br><code>metric_sparse_top_k_categorical_accuracy()</code> <br><code>metric_specificity_at_sensitivity()</code> <br><code>metric_squared_hinge()</code> <br><code>metric_sum()</code> <br><code>metric_top_k_categorical_accuracy()</code> <br><code>metric_true_negatives()</code> <br><code>metric_true_positives()</code> <br></p>


</div>