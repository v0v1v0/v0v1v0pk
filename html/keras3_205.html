<div class="container">

<table style="width: 100%;"><tr>
<td>layer_additive_attention</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Additive attention layer, a.k.a. Bahdanau-style attention.</h2>

<h3>Description</h3>

<p>Inputs are a list with 2 or 3 elements:
</p>

<ol>
<li>
<p> A <code>query</code> tensor of shape <code style="white-space: pre;">⁠(batch_size, Tq, dim)⁠</code>.
</p>
</li>
<li>
<p> A <code>value</code> tensor of shape <code style="white-space: pre;">⁠(batch_size, Tv, dim)⁠</code>.
</p>
</li>
<li>
<p> A optional <code>key</code> tensor of shape <code style="white-space: pre;">⁠(batch_size, Tv, dim)⁠</code>. If none
supplied, <code>value</code> will be used as <code>key</code>.
</p>
</li>
</ol>
<p>The calculation follows the steps:
</p>

<ol>
<li>
<p> Calculate attention scores using <code>query</code> and <code>key</code> with shape
<code style="white-space: pre;">⁠(batch_size, Tq, Tv)⁠</code> as a non-linear sum
<code>scores = reduce_sum(tanh(query + key), axis=-1)</code>.
</p>
</li>
<li>
<p> Use scores to calculate a softmax distribution with shape
<code style="white-space: pre;">⁠(batch_size, Tq, Tv)⁠</code>.
</p>
</li>
<li>
<p> Use the softmax distribution to create a linear combination of <code>value</code>
with shape <code style="white-space: pre;">⁠(batch_size, Tq, dim)⁠</code>.
</p>
</li>
</ol>
<h3>Usage</h3>

<pre><code class="language-R">layer_additive_attention(object, use_scale = TRUE, dropout = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>Object to compose the layer with. A tensor, array, or sequential model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>use_scale</code></td>
<td>
<p>If <code>TRUE</code>, will create a scalar variable to scale the
attention scores.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dropout</code></td>
<td>
<p>Float between 0 and 1. Fraction of the units to drop for the
attention scores. Defaults to <code>0.0</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>For forward/backward compatability.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>The return value depends on the value provided for the first argument.
If  <code>object</code> is:
</p>

<ul>
<li>
<p> a <code>keras_model_sequential()</code>, then the layer is added to the sequential model
(which is modified in place). To enable piping, the sequential model is also
returned, invisibly.
</p>
</li>
<li>
<p> a <code>keras_input()</code>, then the output tensor from calling <code>layer(input)</code> is returned.
</p>
</li>
<li> <p><code>NULL</code> or missing, then a <code>Layer</code> instance is returned.
</p>
</li>
</ul>
<h3>Call Arguments</h3>


<ul>
<li> <p><code>inputs</code>: List of the following tensors:
</p>

<ul>
<li> <p><code>query</code>: Query tensor of shape <code style="white-space: pre;">⁠(batch_size, Tq, dim)⁠</code>.
</p>
</li>
<li> <p><code>value</code>: Value tensor of shape <code style="white-space: pre;">⁠(batch_size, Tv, dim)⁠</code>.
</p>
</li>
<li> <p><code>key</code>: Optional key tensor of shape <code style="white-space: pre;">⁠(batch_size, Tv, dim)⁠</code>. If
not given, will use <code>value</code> for both <code>key</code> and <code>value</code>, which is
the most common case.
</p>
</li>
</ul>
</li>
<li> <p><code>mask</code>: List of the following tensors:
</p>

<ul>
<li> <p><code>query_mask</code>: A boolean mask tensor of shape <code style="white-space: pre;">⁠(batch_size, Tq)⁠</code>.
If given, the output will be zero at the positions where
<code>mask==FALSE</code>.
</p>
</li>
<li> <p><code>value_mask</code>: A boolean mask tensor of shape <code style="white-space: pre;">⁠(batch_size, Tv)⁠</code>.
If given, will apply the mask such that values at positions
where <code>mask==FALSE</code> do not contribute to the result.
</p>
</li>
</ul>
</li>
<li> <p><code>return_attention_scores</code>: bool, it <code>TRUE</code>, returns the attention scores
(after masking and softmax) as an additional output argument.
</p>
</li>
<li> <p><code>training</code>: Python boolean indicating whether the layer should behave in
training mode (adding dropout) or in inference mode (no dropout).
</p>
</li>
<li> <p><code>use_causal_mask</code>: Boolean. Set to <code>TRUE</code> for decoder self-attention. Adds
a mask such that position <code>i</code> cannot attend to positions <code>j &gt; i</code>.
This prevents the flow of information from the future towards the
past. Defaults to <code>FALSE</code>.
</p>
</li>
</ul>
<h3>Output</h3>

<p>Attention outputs of shape <code style="white-space: pre;">⁠(batch_size, Tq, dim)⁠</code>.
(Optional) Attention scores after masking and softmax with shape
<code style="white-space: pre;">⁠(batch_size, Tq, Tv)⁠</code>.
</p>


<h3>See Also</h3>


<ul><li> <p><a href="https://keras.io/api/layers/attention_layers/additive_attention#additiveattention-class">https://keras.io/api/layers/attention_layers/additive_attention#additiveattention-class</a>
</p>
</li></ul>
<p>Other attention layers: <br><code>layer_attention()</code> <br><code>layer_group_query_attention()</code> <br><code>layer_multi_head_attention()</code> <br></p>
<p>Other layers: <br><code>Layer()</code> <br><code>layer_activation()</code> <br><code>layer_activation_elu()</code> <br><code>layer_activation_leaky_relu()</code> <br><code>layer_activation_parametric_relu()</code> <br><code>layer_activation_relu()</code> <br><code>layer_activation_softmax()</code> <br><code>layer_activity_regularization()</code> <br><code>layer_add()</code> <br><code>layer_alpha_dropout()</code> <br><code>layer_attention()</code> <br><code>layer_average()</code> <br><code>layer_average_pooling_1d()</code> <br><code>layer_average_pooling_2d()</code> <br><code>layer_average_pooling_3d()</code> <br><code>layer_batch_normalization()</code> <br><code>layer_bidirectional()</code> <br><code>layer_category_encoding()</code> <br><code>layer_center_crop()</code> <br><code>layer_concatenate()</code> <br><code>layer_conv_1d()</code> <br><code>layer_conv_1d_transpose()</code> <br><code>layer_conv_2d()</code> <br><code>layer_conv_2d_transpose()</code> <br><code>layer_conv_3d()</code> <br><code>layer_conv_3d_transpose()</code> <br><code>layer_conv_lstm_1d()</code> <br><code>layer_conv_lstm_2d()</code> <br><code>layer_conv_lstm_3d()</code> <br><code>layer_cropping_1d()</code> <br><code>layer_cropping_2d()</code> <br><code>layer_cropping_3d()</code> <br><code>layer_dense()</code> <br><code>layer_depthwise_conv_1d()</code> <br><code>layer_depthwise_conv_2d()</code> <br><code>layer_discretization()</code> <br><code>layer_dot()</code> <br><code>layer_dropout()</code> <br><code>layer_einsum_dense()</code> <br><code>layer_embedding()</code> <br><code>layer_feature_space()</code> <br><code>layer_flatten()</code> <br><code>layer_flax_module_wrapper()</code> <br><code>layer_gaussian_dropout()</code> <br><code>layer_gaussian_noise()</code> <br><code>layer_global_average_pooling_1d()</code> <br><code>layer_global_average_pooling_2d()</code> <br><code>layer_global_average_pooling_3d()</code> <br><code>layer_global_max_pooling_1d()</code> <br><code>layer_global_max_pooling_2d()</code> <br><code>layer_global_max_pooling_3d()</code> <br><code>layer_group_normalization()</code> <br><code>layer_group_query_attention()</code> <br><code>layer_gru()</code> <br><code>layer_hashed_crossing()</code> <br><code>layer_hashing()</code> <br><code>layer_identity()</code> <br><code>layer_integer_lookup()</code> <br><code>layer_jax_model_wrapper()</code> <br><code>layer_lambda()</code> <br><code>layer_layer_normalization()</code> <br><code>layer_lstm()</code> <br><code>layer_masking()</code> <br><code>layer_max_pooling_1d()</code> <br><code>layer_max_pooling_2d()</code> <br><code>layer_max_pooling_3d()</code> <br><code>layer_maximum()</code> <br><code>layer_mel_spectrogram()</code> <br><code>layer_minimum()</code> <br><code>layer_multi_head_attention()</code> <br><code>layer_multiply()</code> <br><code>layer_normalization()</code> <br><code>layer_permute()</code> <br><code>layer_random_brightness()</code> <br><code>layer_random_contrast()</code> <br><code>layer_random_crop()</code> <br><code>layer_random_flip()</code> <br><code>layer_random_rotation()</code> <br><code>layer_random_translation()</code> <br><code>layer_random_zoom()</code> <br><code>layer_repeat_vector()</code> <br><code>layer_rescaling()</code> <br><code>layer_reshape()</code> <br><code>layer_resizing()</code> <br><code>layer_rnn()</code> <br><code>layer_separable_conv_1d()</code> <br><code>layer_separable_conv_2d()</code> <br><code>layer_simple_rnn()</code> <br><code>layer_spatial_dropout_1d()</code> <br><code>layer_spatial_dropout_2d()</code> <br><code>layer_spatial_dropout_3d()</code> <br><code>layer_spectral_normalization()</code> <br><code>layer_string_lookup()</code> <br><code>layer_subtract()</code> <br><code>layer_text_vectorization()</code> <br><code>layer_tfsm()</code> <br><code>layer_time_distributed()</code> <br><code>layer_torch_module_wrapper()</code> <br><code>layer_unit_normalization()</code> <br><code>layer_upsampling_1d()</code> <br><code>layer_upsampling_2d()</code> <br><code>layer_upsampling_3d()</code> <br><code>layer_zero_padding_1d()</code> <br><code>layer_zero_padding_2d()</code> <br><code>layer_zero_padding_3d()</code> <br><code>rnn_cell_gru()</code> <br><code>rnn_cell_lstm()</code> <br><code>rnn_cell_simple()</code> <br><code>rnn_cells_stack()</code> <br></p>


</div>