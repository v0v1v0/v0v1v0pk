<div class="container">

<table style="width: 100%;"><tr>
<td>layer_batch_normalization</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Layer that normalizes its inputs</h2>

<h3>Description</h3>

<p>Layer that normalizes its inputs
</p>


<h3>Usage</h3>

<pre><code class="language-R">layer_batch_normalization(
  object,
  axis = -1L,
  momentum = 0.99,
  epsilon = 0.001,
  center = TRUE,
  scale = TRUE,
  beta_initializer = "zeros",
  gamma_initializer = "ones",
  moving_mean_initializer = "zeros",
  moving_variance_initializer = "ones",
  beta_regularizer = NULL,
  gamma_regularizer = NULL,
  beta_constraint = NULL,
  gamma_constraint = NULL,
  synchronized = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>Layer or model object</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>axis</code></td>
<td>
<p>Integer, the axis that should be normalized (typically the features
axis). For instance, after a <code>Conv2D</code> layer with
<code>data_format="channels_first"</code>, set <code>axis=1</code> in <code>BatchNormalization</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>momentum</code></td>
<td>
<p>Momentum for the moving average.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>
<p>Small float added to variance to avoid dividing by zero.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>center</code></td>
<td>
<p>If <code>TRUE</code>, add offset of <code>beta</code> to normalized tensor. If <code>FALSE</code>,
<code>beta</code> is ignored.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scale</code></td>
<td>
<p>If <code>TRUE</code>, multiply by <code>gamma</code>. If <code>FALSE</code>, <code>gamma</code> is not used. When
the next layer is linear (also e.g. <code>nn.relu</code>), this can be disabled
since the scaling will be done by the next layer.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta_initializer</code></td>
<td>
<p>Initializer for the beta weight.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma_initializer</code></td>
<td>
<p>Initializer for the gamma weight.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>moving_mean_initializer</code></td>
<td>
<p>Initializer for the moving mean.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>moving_variance_initializer</code></td>
<td>
<p>Initializer for the moving variance.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta_regularizer</code></td>
<td>
<p>Optional regularizer for the beta weight.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma_regularizer</code></td>
<td>
<p>Optional regularizer for the gamma weight.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta_constraint</code></td>
<td>
<p>Optional constraint for the beta weight.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma_constraint</code></td>
<td>
<p>Optional constraint for the gamma weight.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>synchronized</code></td>
<td>
<p>If <code>TRUE</code>, synchronizes the global batch statistics (mean and
variance) for the layer across all devices at each training step in a
distributed training strategy. If <code>FALSE</code>, each replica uses its own
local batch statistics. Only relevant when used inside a
<code>tf$distribute</code> strategy.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>standard layer arguments.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Batch normalization applies a transformation that maintains the mean output
close to 0 and the output standard deviation close to 1.
</p>
<p>Importantly, batch normalization works differently during training and
during inference.
</p>
<p><strong>During training</strong> (i.e. when using <code>fit()</code> or when calling the layer/model
with the argument <code>training=TRUE</code>), the layer normalizes its output using
the mean and standard deviation of the current batch of inputs. That is to
say, for each channel being normalized, the layer returns
<code>gamma * (batch - mean(batch)) / sqrt(var(batch) + epsilon) + beta</code>, where:
</p>

<ul>
<li> <p><code>epsilon</code> is small constant (configurable as part of the constructor
arguments)
</p>
</li>
<li> <p><code>gamma</code> is a learned scaling factor (initialized as 1), which
can be disabled by passing <code>scale=FALSE</code> to the constructor.
</p>
</li>
<li> <p><code>beta</code> is a learned offset factor (initialized as 0), which
can be disabled by passing <code>center=FALSE</code> to the constructor.
</p>
</li>
</ul>
<p><strong>During inference</strong> (i.e. when using <code>evaluate()</code> or <code>predict()</code> or when
calling the layer/model with the argument <code>training=FALSE</code> (which is the
default), the layer normalizes its output using a moving average of the
mean and standard deviation of the batches it has seen during training. That
is to say, it returns
<code>gamma * (batch - self.moving_mean) / sqrt(self.moving_var+epsilon) + beta</code>.
</p>
<p><code>self$moving_mean</code> and <code>self$moving_var</code> are non-trainable variables that
are updated each time the layer in called in training mode, as such:
</p>

<ul>
<li> <p><code>moving_mean = moving_mean * momentum + mean(batch) * (1 - momentum)</code>
</p>
</li>
<li> <p><code>moving_var = moving_var * momentum + var(batch) * (1 - momentum)</code>
</p>
</li>
</ul>
<p>As such, the layer will only normalize its inputs during inference
<em>after having been trained on data that has similar statistics as the
inference data</em>.
</p>
<p>When <code>synchronized=TRUE</code> is set and if this layer is used within a
<code>tf$distribute</code> strategy, there will be an <code>allreduce</code> call
to aggregate batch statistics across all replicas at every
training step. Setting <code>synchronized</code> has no impact when the model is
trained without specifying any distribution strategy.
</p>
<p>Example usage:
</p>
<div class="sourceCode R"><pre>strategy &lt;- tf$distribute$MirroredStrategy()

with(strategy$scope(), {
  model &lt;- keras_model_sequential()
  model %&gt;%
    layer_dense(16) %&gt;%
    layer_batch_normalization(synchronized=TRUE)
})
</pre></div>


<h3>See Also</h3>


<ul>
<li> <p><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization">https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization</a>
</p>
</li>
<li> <p><a href="https://keras.io/api/layers">https://keras.io/api/layers</a>
</p>
</li>
</ul>
</div>