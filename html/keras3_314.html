<div class="container">

<table style="width: 100%;"><tr>
<td>loss_binary_crossentropy</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Computes the cross-entropy loss between true labels and predicted labels.</h2>

<h3>Description</h3>

<p>Use this cross-entropy loss for binary (0 or 1) classification applications.
The loss function requires the following inputs:
</p>

<ul>
<li> <p><code>y_true</code> (true label): This is either 0 or 1.
</p>
</li>
<li> <p><code>y_pred</code> (predicted value): This is the model's prediction, i.e, a single
floating-point value which either represents a
<a href="https://en.wikipedia.org/wiki/Logit">logit</a>, (i.e, value in <code style="white-space: pre;">⁠[-inf, inf]⁠</code>
when <code>from_logits=TRUE</code>) or a probability (i.e, value in <code style="white-space: pre;">⁠[0., 1.]⁠</code> when
<code>from_logits=FALSE</code>).
</p>
</li>
</ul>
<h3>Usage</h3>

<pre><code class="language-R">loss_binary_crossentropy(
  y_true,
  y_pred,
  from_logits = FALSE,
  label_smoothing = 0,
  axis = -1L,
  ...,
  reduction = "sum_over_batch_size",
  name = "binary_crossentropy",
  dtype = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y_true</code></td>
<td>
<p>Ground truth values. shape = <code style="white-space: pre;">⁠[batch_size, d0, .. dN]⁠</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y_pred</code></td>
<td>
<p>The predicted values. shape = <code style="white-space: pre;">⁠[batch_size, d0, .. dN]⁠</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>from_logits</code></td>
<td>
<p>Whether to interpret <code>y_pred</code> as a tensor of
<a href="https://en.wikipedia.org/wiki/Logit">logit</a> values. By default, we
assume that <code>y_pred</code> is probabilities (i.e., values in <code style="white-space: pre;">⁠[0, 1)).⁠</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>label_smoothing</code></td>
<td>
<p>Float in range <code style="white-space: pre;">⁠[0, 1].⁠</code> When 0, no smoothing occurs.
When &gt; 0, we compute the loss between the predicted labels
and a smoothed version of the true labels, where the smoothing
squeezes the labels towards 0.5. Larger values of
<code>label_smoothing</code> correspond to heavier smoothing.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>axis</code></td>
<td>
<p>The axis along which to compute crossentropy (the features axis).
Defaults to <code>-1</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>For forward/backward compatability.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>reduction</code></td>
<td>
<p>Type of reduction to apply to the loss. In almost all cases
this should be <code>"sum_over_batch_size"</code>.
Supported options are <code>"sum"</code>, <code>"sum_over_batch_size"</code> or <code>NULL</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>name</code></td>
<td>
<p>Optional name for the loss instance.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dtype</code></td>
<td>
<p>The dtype of the loss's computations. Defaults to <code>NULL</code>, which
means using <code>config_floatx()</code>. <code>config_floatx()</code> is a
<code>"float32"</code> unless set to different value
(via <code>config_set_floatx()</code>). If a <code>keras$DTypePolicy</code> is
provided, then the <code>compute_dtype</code> will be utilized.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>Binary crossentropy loss value. shape = <code style="white-space: pre;">⁠[batch_size, d0, .. dN-1]⁠</code>.
</p>


<h3>Examples</h3>

<div class="sourceCode r"><pre>y_true &lt;- rbind(c(0, 1), c(0, 0))
y_pred &lt;- rbind(c(0.6, 0.4), c(0.4, 0.6))
loss &lt;- loss_binary_crossentropy(y_true, y_pred)
loss
</pre></div>
<div class="sourceCode"><pre>## tf.Tensor([0.91629073 0.71355818], shape=(2), dtype=float64)

</pre></div>
<p><strong>Recommended Usage:</strong> (set <code>from_logits=TRUE</code>)
</p>
<p>With <code>compile()</code> API:
</p>
<div class="sourceCode r"><pre>model %&gt;% compile(
    loss = loss_binary_crossentropy(from_logits=TRUE),
    ...
)
</pre></div>
<p>As a standalone function:
</p>
<div class="sourceCode r"><pre># Example 1: (batch_size = 1, number of samples = 4)
y_true &lt;- op_array(c(0, 1, 0, 0))
y_pred &lt;- op_array(c(-18.6, 0.51, 2.94, -12.8))
bce &lt;- loss_binary_crossentropy(from_logits = TRUE)
bce(y_true, y_pred)
</pre></div>
<div class="sourceCode"><pre>## tf.Tensor(0.865458, shape=(), dtype=float32)

</pre></div>
<div class="sourceCode r"><pre># Example 2: (batch_size = 2, number of samples = 4)
y_true &lt;- rbind(c(0, 1), c(0, 0))
y_pred &lt;- rbind(c(-18.6, 0.51), c(2.94, -12.8))
# Using default 'auto'/'sum_over_batch_size' reduction type.
bce &lt;- loss_binary_crossentropy(from_logits = TRUE)
bce(y_true, y_pred)
</pre></div>
<div class="sourceCode"><pre>## tf.Tensor(0.865458, shape=(), dtype=float32)

</pre></div>
<div class="sourceCode r"><pre># Using 'sample_weight' attribute
bce(y_true, y_pred, sample_weight = c(0.8, 0.2))
</pre></div>
<div class="sourceCode"><pre>## tf.Tensor(0.2436386, shape=(), dtype=float32)

</pre></div>
<div class="sourceCode r"><pre># 0.243
# Using 'sum' reduction` type.
bce &lt;- loss_binary_crossentropy(from_logits = TRUE, reduction = "sum")
bce(y_true, y_pred)
</pre></div>
<div class="sourceCode"><pre>## tf.Tensor(1.730916, shape=(), dtype=float32)

</pre></div>
<div class="sourceCode r"><pre># Using 'none' reduction type.
bce &lt;- loss_binary_crossentropy(from_logits = TRUE, reduction = NULL)
bce(y_true, y_pred)
</pre></div>
<div class="sourceCode"><pre>## tf.Tensor([0.23515666 1.4957594 ], shape=(2), dtype=float32)

</pre></div>
<p><strong>Default Usage:</strong> (set <code>from_logits=FALSE</code>)
</p>
<div class="sourceCode r"><pre># Make the following updates to the above "Recommended Usage" section
# 1. Set `from_logits=FALSE`
loss_binary_crossentropy() # OR ...('from_logits=FALSE')
</pre></div>
<div class="sourceCode"><pre>## &lt;keras.src.losses.losses.BinaryCrossentropy object&gt;
##  signature: (y_true, y_pred, sample_weight=None)

</pre></div>
<div class="sourceCode r"><pre># 2. Update `y_pred` to use probabilities instead of logits
y_pred &lt;- c(0.6, 0.3, 0.2, 0.8) # OR [[0.6, 0.3], [0.2, 0.8]]
</pre></div>


<h3>See Also</h3>


<ul><li> <p><a href="https://keras.io/api/losses/probabilistic_losses#binarycrossentropy-class">https://keras.io/api/losses/probabilistic_losses#binarycrossentropy-class</a>
</p>
</li></ul>
<p>Other losses: <br><code>Loss()</code> <br><code>loss_binary_focal_crossentropy()</code> <br><code>loss_categorical_crossentropy()</code> <br><code>loss_categorical_focal_crossentropy()</code> <br><code>loss_categorical_hinge()</code> <br><code>loss_cosine_similarity()</code> <br><code>loss_ctc()</code> <br><code>loss_dice()</code> <br><code>loss_hinge()</code> <br><code>loss_huber()</code> <br><code>loss_kl_divergence()</code> <br><code>loss_log_cosh()</code> <br><code>loss_mean_absolute_error()</code> <br><code>loss_mean_absolute_percentage_error()</code> <br><code>loss_mean_squared_error()</code> <br><code>loss_mean_squared_logarithmic_error()</code> <br><code>loss_poisson()</code> <br><code>loss_sparse_categorical_crossentropy()</code> <br><code>loss_squared_hinge()</code> <br><code>loss_tversky()</code> <br><code>metric_binary_crossentropy()</code> <br><code>metric_binary_focal_crossentropy()</code> <br><code>metric_categorical_crossentropy()</code> <br><code>metric_categorical_focal_crossentropy()</code> <br><code>metric_categorical_hinge()</code> <br><code>metric_hinge()</code> <br><code>metric_huber()</code> <br><code>metric_kl_divergence()</code> <br><code>metric_log_cosh()</code> <br><code>metric_mean_absolute_error()</code> <br><code>metric_mean_absolute_percentage_error()</code> <br><code>metric_mean_squared_error()</code> <br><code>metric_mean_squared_logarithmic_error()</code> <br><code>metric_poisson()</code> <br><code>metric_sparse_categorical_crossentropy()</code> <br><code>metric_squared_hinge()</code> <br></p>


</div>