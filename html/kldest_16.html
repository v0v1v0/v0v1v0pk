<div class="container">

<table style="width: 100%;"><tr>
<td>kld_est_nn</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>k-nearest neighbour KL divergence estimator</h2>

<h3>Description</h3>

<p>This function estimates Kullback-Leibler divergence <code class="reqn">D_{KL}(P||Q)</code> between
two continuous distributions <code class="reqn">P</code> and <code class="reqn">Q</code> using nearest-neighbour (NN)
density estimation in a Monte Carlo approximation of <code class="reqn">D_{KL}(P||Q)</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">kld_est_nn(X, Y = NULL, q = NULL, k = 1L, eps = 0, log.q = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X, Y</code></td>
<td>
<p><code>n</code>-by-<code>d</code> and <code>m</code>-by-<code>d</code> matrices, representing <code>n</code> samples from
the true distribution <code class="reqn">P</code> and <code>m</code> samples from the approximate distribution
<code class="reqn">Q</code>, both in <code>d</code> dimensions. Vector input is treated as a column matrix.
<code>Y</code> can be left blank if <code>q</code> is specified (see below).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>q</code></td>
<td>
<p>The density function of the approximate distribution <code class="reqn">Q</code>. Either
<code>Y</code> or <code>q</code> must be specified.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>The number of nearest neighbours to consider for NN density estimation.
Larger values for <code>k</code> generally increase bias, but decrease variance of the
estimator. Defaults to <code>k = 1</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>Error bound in the nearest neighbour search. A value of <code>eps = 0</code>
(the default) implies an exact nearest neighbour search, for <code>eps &gt; 0</code>
approximate nearest neighbours are sought, which may be somewhat faster for
high-dimensional problems.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>log.q</code></td>
<td>
<p>If <code>TRUE</code>, function <code>q</code> is the log-density rather than the density
of the approximate distribution <code class="reqn">Q</code> (default: <code>log.q = FALSE</code>).</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Input for estimation is a sample <code>X</code> from <code class="reqn">P</code> and either the density
function <code>q</code> of <code class="reqn">Q</code> (one-sample problem) or a sample <code>Y</code> of <code class="reqn">Q</code>
(two-sample problem). In the two-sample problem, it is the estimator in Eq.(5)
of Wang et al. (2009). In the one-sample problem, the asymptotic bias (the
expectation of a Gamma distribution) is substracted, see Pérez-Cruz (2008),
Eq.(18).
</p>
<p>References:
</p>
<p>Wang, Kulkarni and Verdú, "Divergence Estimation for Multidimensional
Densities Via k-Nearest-Neighbor Distances", IEEE Transactions on Information
Theory, Vol. 55, No. 5 (2009).
</p>
<p>Pérez-Cruz, "Kullback-Leibler Divergence Estimation of Continuous
Distributions", IEEE International Symposium on Information Theory (2008).
</p>


<h3>Value</h3>

<p>A scalar, the estimated Kullback-Leibler divergence <code class="reqn">\hat D_{KL}(P||Q)</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># KL-D between one or two samples from 1-D Gaussians:
set.seed(0)
X &lt;- rnorm(100)
Y &lt;- rnorm(100, mean = 1, sd = 2)
q &lt;- function(x) dnorm(x, mean = 1, sd =2)
kld_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2)
kld_est_nn(X, Y)
kld_est_nn(X, q = q)
kld_est_nn(X, Y, k = 5)
kld_est_nn(X, q = q, k = 5)
kld_est_brnn(X, Y)


# KL-D between two samples from 2-D Gaussians:
set.seed(0)
X1 &lt;- rnorm(100)
X2 &lt;- rnorm(100)
Y1 &lt;- rnorm(100)
Y2 &lt;- Y1 + rnorm(100)
X &lt;- cbind(X1,X2)
Y &lt;- cbind(Y1,Y2)
kld_gaussian(mu1 = rep(0,2), sigma1 = diag(2),
             mu2 = rep(0,2), sigma2 = matrix(c(1,1,1,2),nrow=2))
kld_est_nn(X, Y)
kld_est_nn(X, Y, k = 5)
kld_est_brnn(X, Y)
</code></pre>


</div>