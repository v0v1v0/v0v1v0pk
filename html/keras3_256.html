<div class="container">

<table style="width: 100%;"><tr>
<td>layer_jax_model_wrapper</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Keras Layer that wraps a JAX model.</h2>

<h3>Description</h3>

<p>This layer enables the use of JAX components within Keras when using JAX as
the backend for Keras.
</p>


<h3>Usage</h3>

<pre><code class="language-R">layer_jax_model_wrapper(
  object,
  call_fn,
  init_fn = NULL,
  params = NULL,
  state = NULL,
  seed = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>Object to compose the layer with. A tensor, array, or sequential model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>call_fn</code></td>
<td>
<p>The function to call the model. See description above for the
list of arguments it takes and the outputs it returns.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>init_fn</code></td>
<td>
<p>the function to call to initialize the model. See description
above for the list of arguments it takes and the ouputs it returns.
If <code>NULL</code>, then <code>params</code> and/or <code>state</code> must be provided.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>params</code></td>
<td>
<p>A <code>PyTree</code> containing all the model trainable parameters. This
allows passing trained parameters or controlling the initialization.
If both <code>params</code> and <code>state</code> are <code>NULL</code>, <code>init_fn()</code> is called at
build time to initialize the trainable parameters of the model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>state</code></td>
<td>
<p>A <code>PyTree</code> containing all the model non-trainable state. This
allows passing learned state or controlling the initialization. If
both <code>params</code> and <code>state</code> are <code>NULL</code>, and <code>call_fn()</code> takes a <code>state</code>
argument, then <code>init_fn()</code> is called at build time to initialize the
non-trainable state of the model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p>Seed for random number generator. Optional.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>For forward/backward compatability.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>The return value depends on the value provided for the first argument.
If  <code>object</code> is:
</p>

<ul>
<li>
<p> a <code>keras_model_sequential()</code>, then the layer is added to the sequential model
(which is modified in place). To enable piping, the sequential model is also
returned, invisibly.
</p>
</li>
<li>
<p> a <code>keras_input()</code>, then the output tensor from calling <code>layer(input)</code> is returned.
</p>
</li>
<li> <p><code>NULL</code> or missing, then a <code>Layer</code> instance is returned.
</p>
</li>
</ul>
<h3>Model function</h3>

<p>This layer accepts JAX models in the form of a function, <code>call_fn()</code>, which
must take the following arguments with these exact names:
</p>

<ul>
<li> <p><code>params</code>: trainable parameters of the model.
</p>
</li>
<li> <p><code>state</code> (<em>optional</em>): non-trainable state of the model. Can be omitted if
the model has no non-trainable state.
</p>
</li>
<li> <p><code>rng</code> (<em>optional</em>): a <code>jax.random.PRNGKey</code> instance. Can be omitted if the
model does not need RNGs, neither during training nor during inference.
</p>
</li>
<li> <p><code>inputs</code>: inputs to the model, a JAX array or a <code>PyTree</code> of arrays.
</p>
</li>
<li> <p><code>training</code> (<em>optional</em>): an argument specifying if we're in training mode
or inference mode, <code>TRUE</code> is passed in training mode. Can be omitted if
the model behaves the same in training mode and inference mode.
</p>
</li>
</ul>
<p>The <code>inputs</code> argument is mandatory. Inputs to the model must be provided via
a single argument. If the JAX model takes multiple inputs as separate
arguments, they must be combined into a single structure, for instance in a
<code>tuple()</code> or a <code>dict()</code>.
</p>


<h4>Model weights initialization</h4>

<p>The initialization of the <code>params</code> and <code>state</code> of the model can be handled
by this layer, in which case the <code>init_fn()</code> argument must be provided. This
allows the model to be initialized dynamically with the right shape.
Alternatively, and if the shape is known, the <code>params</code> argument and
optionally the <code>state</code> argument can be used to create an already initialized
model.
</p>
<p>The <code>init_fn()</code> function, if provided, must take the following arguments with
these exact names:
</p>

<ul>
<li> <p><code>rng</code>: a <code>jax.random.PRNGKey</code> instance.
</p>
</li>
<li> <p><code>inputs</code>: a JAX array or a <code>PyTree</code> of arrays with placeholder values to
provide the shape of the inputs.
</p>
</li>
<li> <p><code>training</code> (<em>optional</em>): an argument specifying if we're in training mode
or inference mode. <code>True</code> is always passed to <code>init_fn</code>. Can be omitted
regardless of whether <code>call_fn</code> has a <code>training</code> argument.
</p>
</li>
</ul>
<h4>Models with non-trainable state</h4>

<p>For JAX models that have non-trainable state:
</p>

<ul>
<li> <p><code>call_fn()</code> must have a <code>state</code> argument
</p>
</li>
<li> <p><code>call_fn()</code> must return a <code>tuple()</code> containing the outputs of the model and
the new non-trainable state of the model
</p>
</li>
<li> <p><code>init_fn()</code> must return a <code>tuple()</code> containing the initial trainable params of
the model and the initial non-trainable state of the model.
</p>
</li>
</ul>
<p>This code shows a possible combination of <code>call_fn()</code> and <code>init_fn()</code> signatures
for a model with non-trainable state. In this example, the model has a
<code>training</code> argument and an <code>rng</code> argument in <code>call_fn()</code>.
</p>
<div class="sourceCode r"><pre>stateful_call &lt;- function(params, state, rng, inputs, training) {
  outputs &lt;- ....
  new_state &lt;- ....
  tuple(outputs, new_state)
}

stateful_init &lt;- function(rng, inputs) {
  initial_params &lt;- ....
  initial_state &lt;- ....
  tuple(initial_params, initial_state)
}
</pre></div>



<h4>Models without non-trainable state</h4>

<p>For JAX models with no non-trainable state:
</p>

<ul>
<li> <p><code>call_fn()</code> must not have a <code>state</code> argument
</p>
</li>
<li> <p><code>call_fn()</code> must return only the outputs of the model
</p>
</li>
<li> <p><code>init_fn()</code> must return only the initial trainable params of the model.
</p>
</li>
</ul>
<p>This code shows a possible combination of <code>call_fn()</code> and <code>init_fn()</code> signatures
for a model without non-trainable state. In this example, the model does not
have a <code>training</code> argument and does not have an <code>rng</code> argument in <code>call_fn()</code>.
</p>
<div class="sourceCode r"><pre>stateful_call &lt;- function(pparams, inputs) {
  outputs &lt;- ....
  outputs
}

stateful_init &lt;- function(rng, inputs) {
  initial_params &lt;- ....
  initial_params
}
</pre></div>



<h4>Conforming to the required signature</h4>

<p>If a model has a different signature than the one required by <code>JaxLayer</code>,
one can easily write a wrapper method to adapt the arguments. This example
shows a model that has multiple inputs as separate arguments, expects
multiple RNGs in a <code>dict</code>, and has a <code>deterministic</code> argument with the
opposite meaning of <code>training</code>. To conform, the inputs are combined in a
single structure using a <code>tuple</code>, the RNG is split and used the populate the
expected <code>dict</code>, and the Boolean flag is negated:
</p>
<div class="sourceCode r"><pre>jax &lt;- import("jax")
my_model_fn &lt;- function(params, rngs, input1, input2, deterministic) {
  ....
  if (!deterministic) {
    dropout_rng &lt;- rngs$dropout
    keep &lt;- jax$random$bernoulli(dropout_rng, dropout_rate, x$shape)
    x &lt;- jax$numpy$where(keep, x / dropout_rate, 0)
    ....
  }
  ....
  return(outputs)
}

my_model_wrapper_fn &lt;- function(params, rng, inputs, training) {
  c(input1, input2) %&lt;-% inputs
  c(rng1, rng2) %&lt;-% jax$random$split(rng)
  rngs &lt;-  list(dropout = rng1, preprocessing = rng2)
  deterministic &lt;-  !training
  my_model_fn(params, rngs, input1, input2, deterministic)
}

keras_layer &lt;- layer_jax_model_wrapper(call_fn = my_model_wrapper_fn,
                                       params = initial_params)
</pre></div>



<h4>Usage with Haiku modules</h4>

<p><code>JaxLayer</code> enables the use of <a href="https://dm-haiku.readthedocs.io">Haiku</a>
components in the form of
<a href="https://dm-haiku.readthedocs.io/en/latest/api.html#module"><code>haiku.Module</code></a>.
This is achieved by transforming the module per the Haiku pattern and then
passing <code>module.apply</code> in the <code>call_fn</code> parameter and <code>module.init</code> in the
<code>init_fn</code> parameter if needed.
</p>
<p>If the model has non-trainable state, it should be transformed with
<a href="https://dm-haiku.readthedocs.io/en/latest/api.html#haiku.transform_with_state"><code>haiku.transform_with_state</code></a>.
If the model has no non-trainable state, it should be transformed with
<a href="https://dm-haiku.readthedocs.io/en/latest/api.html#haiku.transform"><code>haiku.transform</code></a>.
Additionally, and optionally, if the module does not use RNGs in "apply", it
can be transformed with
<a href="https://dm-haiku.readthedocs.io/en/latest/api.html#without-apply-rng"><code>haiku.without_apply_rng</code></a>.
</p>
<p>The following example shows how to create a <code>JaxLayer</code> from a Haiku module
that uses random number generators via <code>hk.next_rng_key()</code> and takes a
training positional argument:
</p>
<div class="sourceCode r"><pre># reticulate::py_install("haiku", "r-keras")
hk &lt;- import("haiku")
MyHaikuModule(hk$Module) \%py_class\% {

  `__call__` &lt;- \(self, x, training) {
    x &lt;- hk$Conv2D(32L, tuple(3L, 3L))(x)
    x &lt;- jax$nn$relu(x)
    x &lt;- hk$AvgPool(tuple(1L, 2L, 2L, 1L),
                    tuple(1L, 2L, 2L, 1L), "VALID")(x)
    x &lt;- hk$Flatten()(x)
    x &lt;- hk$Linear(200L)(x)
    if (training)
      x &lt;- hk$dropout(rng = hk$next_rng_key(), rate = 0.3, x = x)
    x &lt;- jax$nn$relu(x)
    x &lt;- hk$Linear(10L)(x)
    x &lt;- jax$nn$softmax(x)
    x
  }

}

my_haiku_module_fn &lt;- function(inputs, training) {
  module &lt;- MyHaikuModule()
  module(inputs, training)
}

transformed_module &lt;- hk$transform(my_haiku_module_fn)

keras_layer &lt;-
  layer_jax_model_wrapper(call_fn = transformed_module$apply,
                          init_fn = transformed_module$init)
</pre></div>



<h3>See Also</h3>

<p>Other wrapping layers: <br><code>layer_flax_module_wrapper()</code> <br><code>layer_torch_module_wrapper()</code> <br></p>
<p>Other layers: <br><code>Layer()</code> <br><code>layer_activation()</code> <br><code>layer_activation_elu()</code> <br><code>layer_activation_leaky_relu()</code> <br><code>layer_activation_parametric_relu()</code> <br><code>layer_activation_relu()</code> <br><code>layer_activation_softmax()</code> <br><code>layer_activity_regularization()</code> <br><code>layer_add()</code> <br><code>layer_additive_attention()</code> <br><code>layer_alpha_dropout()</code> <br><code>layer_attention()</code> <br><code>layer_average()</code> <br><code>layer_average_pooling_1d()</code> <br><code>layer_average_pooling_2d()</code> <br><code>layer_average_pooling_3d()</code> <br><code>layer_batch_normalization()</code> <br><code>layer_bidirectional()</code> <br><code>layer_category_encoding()</code> <br><code>layer_center_crop()</code> <br><code>layer_concatenate()</code> <br><code>layer_conv_1d()</code> <br><code>layer_conv_1d_transpose()</code> <br><code>layer_conv_2d()</code> <br><code>layer_conv_2d_transpose()</code> <br><code>layer_conv_3d()</code> <br><code>layer_conv_3d_transpose()</code> <br><code>layer_conv_lstm_1d()</code> <br><code>layer_conv_lstm_2d()</code> <br><code>layer_conv_lstm_3d()</code> <br><code>layer_cropping_1d()</code> <br><code>layer_cropping_2d()</code> <br><code>layer_cropping_3d()</code> <br><code>layer_dense()</code> <br><code>layer_depthwise_conv_1d()</code> <br><code>layer_depthwise_conv_2d()</code> <br><code>layer_discretization()</code> <br><code>layer_dot()</code> <br><code>layer_dropout()</code> <br><code>layer_einsum_dense()</code> <br><code>layer_embedding()</code> <br><code>layer_feature_space()</code> <br><code>layer_flatten()</code> <br><code>layer_flax_module_wrapper()</code> <br><code>layer_gaussian_dropout()</code> <br><code>layer_gaussian_noise()</code> <br><code>layer_global_average_pooling_1d()</code> <br><code>layer_global_average_pooling_2d()</code> <br><code>layer_global_average_pooling_3d()</code> <br><code>layer_global_max_pooling_1d()</code> <br><code>layer_global_max_pooling_2d()</code> <br><code>layer_global_max_pooling_3d()</code> <br><code>layer_group_normalization()</code> <br><code>layer_group_query_attention()</code> <br><code>layer_gru()</code> <br><code>layer_hashed_crossing()</code> <br><code>layer_hashing()</code> <br><code>layer_identity()</code> <br><code>layer_integer_lookup()</code> <br><code>layer_lambda()</code> <br><code>layer_layer_normalization()</code> <br><code>layer_lstm()</code> <br><code>layer_masking()</code> <br><code>layer_max_pooling_1d()</code> <br><code>layer_max_pooling_2d()</code> <br><code>layer_max_pooling_3d()</code> <br><code>layer_maximum()</code> <br><code>layer_mel_spectrogram()</code> <br><code>layer_minimum()</code> <br><code>layer_multi_head_attention()</code> <br><code>layer_multiply()</code> <br><code>layer_normalization()</code> <br><code>layer_permute()</code> <br><code>layer_random_brightness()</code> <br><code>layer_random_contrast()</code> <br><code>layer_random_crop()</code> <br><code>layer_random_flip()</code> <br><code>layer_random_rotation()</code> <br><code>layer_random_translation()</code> <br><code>layer_random_zoom()</code> <br><code>layer_repeat_vector()</code> <br><code>layer_rescaling()</code> <br><code>layer_reshape()</code> <br><code>layer_resizing()</code> <br><code>layer_rnn()</code> <br><code>layer_separable_conv_1d()</code> <br><code>layer_separable_conv_2d()</code> <br><code>layer_simple_rnn()</code> <br><code>layer_spatial_dropout_1d()</code> <br><code>layer_spatial_dropout_2d()</code> <br><code>layer_spatial_dropout_3d()</code> <br><code>layer_spectral_normalization()</code> <br><code>layer_string_lookup()</code> <br><code>layer_subtract()</code> <br><code>layer_text_vectorization()</code> <br><code>layer_tfsm()</code> <br><code>layer_time_distributed()</code> <br><code>layer_torch_module_wrapper()</code> <br><code>layer_unit_normalization()</code> <br><code>layer_upsampling_1d()</code> <br><code>layer_upsampling_2d()</code> <br><code>layer_upsampling_3d()</code> <br><code>layer_zero_padding_1d()</code> <br><code>layer_zero_padding_2d()</code> <br><code>layer_zero_padding_3d()</code> <br><code>rnn_cell_gru()</code> <br><code>rnn_cell_lstm()</code> <br><code>rnn_cell_simple()</code> <br><code>rnn_cells_stack()</code> <br></p>


</div>